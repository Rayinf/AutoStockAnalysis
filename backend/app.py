from fastapi import FastAPI, HTTPException, Query
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from dotenv import load_dotenv, find_dotenv
import os

load_dotenv(find_dotenv())

# ç”Ÿäº§ç¯å¢ƒé…ç½®
ENVIRONMENT = os.getenv('ENVIRONMENT', 'development')
ALLOWED_ORIGINS = os.getenv('ALLOWED_ORIGINS', '*').split(',')
PORT = int(os.getenv('PORT', 8000))

from Models.Factory import ChatModelFactory
from Tools.stock_quote import StockAnalyser, StockTool
from backend.qlib_provider import QlibProvider
from backend.feature_store_cache import feature_store
from backend.features import build_factors
from backend.strategy_backtest import backtester, StrategyConfig
import traceback
import logging

# é…ç½®æ—¥å¿—
logger = logging.getLogger(__name__)
import sqlite3
from typing import List, Dict, Any, Optional
from fastapi.responses import StreamingResponse

# å¯¼å…¥æŒä»“ç®¡ç†æ¨¡å—
from backend.portfolio_manager import portfolio_manager, Position, Transaction, Portfolio

try:
    import akshare as ak
    import pandas as pd
    import numpy as np
except Exception:
    ak = None
    pd = None
    np = None

# Qlibé›†æˆ
try:
    from backend.qlib_demo import demo_qlib_integration, QlibDataAdapter, QlibPredictor
    QLIB_AVAILABLE = True
except Exception:
    QLIB_AVAILABLE = False

app = FastAPI(title="Auto-GPT-Stock API", version="0.1.0")

# CORSé…ç½® - å…è®¸GitHub Pageså’Œæœ¬åœ°å¼€å‘è®¿é—®
if ENVIRONMENT == 'production':
    # ç”Ÿäº§ç¯å¢ƒï¼šå…è®¸GitHub PagesåŸŸåå’ŒRailwayåŸŸå
    cors_origins = [
        "https://rayinf.github.io",  # GitHub PagesåŸŸå
        "https://autostockanalysis-production.up.railway.app",  # RailwayåŸŸå
        "http://localhost:5173",  # æœ¬åœ°å¼€å‘
        "http://127.0.0.1:5173",  # æœ¬åœ°å¼€å‘
    ]
    # å¦‚æœè®¾ç½®äº†ALLOWED_ORIGINSç¯å¢ƒå˜é‡ï¼Œåˆ™åˆå¹¶ä½¿ç”¨
    if ALLOWED_ORIGINS != ['*']:
        cors_origins.extend(ALLOWED_ORIGINS)
else:
    # å¼€å‘ç¯å¢ƒï¼šå…è®¸æ‰€æœ‰æ¥æº
    cors_origins = ["*"]

app.add_middleware(
    CORSMiddleware,
    allow_origins=cors_origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# åˆå§‹åŒ–QlibProviderï¼ˆæƒ°æ€§å•ä¾‹ï¼Œå¯åœ¨ /api/qlib/status æŸ¥è¯¢ï¼‰
_qlib = QlibProvider.get()

# è‡ªå®šä¹‰JSONç¼–ç å™¨æ¥å¤„ç†NaNå’Œæ—¥æœŸ
from fastapi.responses import JSONResponse
import json as _json
from datetime import datetime, date
import numpy as np

class CustomJSONEncoder(_json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, (datetime, date)):
            return obj.isoformat()
        import math
        # å¤„ç†Python float NaN/inf
        if isinstance(obj, float):
            if math.isnan(obj) or math.isinf(obj):
                return None
            return obj
        # å¤„ç†numpyæ•°å€¼ç±»å‹å’ŒNaNå€¼
        if hasattr(obj, 'dtype') and np.isscalar(obj):
            if np.isnan(obj) or np.isinf(obj):
                return None
            return float(obj)
        return super().default(obj)

# é¢„å¤„ç†å‡½æ•°æ¥æ¸…ç†NaNå€¼
def clean_nan_values(obj):
    """é€’å½’æ¸…ç†æ•°æ®ç»“æ„ä¸­çš„NaNå’ŒInfinityå€¼"""
    import math
    if isinstance(obj, dict):
        return {k: clean_nan_values(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [clean_nan_values(item) for item in obj]
    elif isinstance(obj, float):
        if math.isnan(obj) or math.isinf(obj):
            return None
        return obj
    elif hasattr(obj, 'dtype') and np.isscalar(obj):
        if np.isnan(obj) or np.isinf(obj):
            return None
        return float(obj)
    else:
        return obj

# è¦†ç›–é»˜è®¤çš„JSONå“åº”
def custom_json_response(content, **kwargs):
    cleaned_content = clean_nan_values(content)
    return JSONResponse(
        content=cleaned_content,
        **kwargs
    )

def load_predictions_for_enhanced_strategy(symbol_list, start_date, end_date, days):
    """ä¸ºå¢å¼ºç­–ç•¥åŠ è½½é¢„æµ‹æ•°æ®"""
    try:
        # ä½¿ç”¨ç°æœ‰çš„æ ¡å‡†å™¨åŠ è½½é¢„æµ‹æ•°æ®
        calibrator_instance = HistoricalBacktestCalibrator()
        
        # è·å–é¢„æµ‹æ•°æ®
        predictions = calibrator_instance.load_predictions()
        
        if predictions.empty:
            return predictions
            
        # ç­›é€‰è‚¡ç¥¨
        if symbol_list:
            predictions = predictions[predictions['symbol'].isin(symbol_list)]
        
        # ç­›é€‰æ—¥æœŸ
        if start_date:
            predictions = predictions[predictions['prediction_date'] >= start_date]
        if end_date:
            predictions = predictions[predictions['prediction_date'] <= end_date]
        elif days:
            # å–æœ€è¿‘Nå¤©çš„æ•°æ®
            latest_date = predictions['prediction_date'].max()
            start_date = latest_date - pd.Timedelta(days=days)
            predictions = predictions[predictions['prediction_date'] >= start_date]
        
        return predictions
        
    except Exception as e:
        logger.error(f"åŠ è½½é¢„æµ‹æ•°æ®å¤±è´¥: {e}")
        return pd.DataFrame()

def load_price_data_for_enhanced_strategy(symbol_list, start_date, end_date, days):
    """ä¸ºå¢å¼ºç­–ç•¥åŠ è½½ä»·æ ¼æ•°æ®"""
    try:
        from backend.data_manager import get_stock_data
        from datetime import datetime, timedelta
        
        all_price_data = []
        
        if not symbol_list:
            return pd.DataFrame()
        
        for symbol in symbol_list:
            try:
                # è®¡ç®—æ—¥æœŸèŒƒå›´
                if start_date and end_date:
                    price_start = start_date
                    price_end = end_date
                elif days:
                    end_dt = datetime.now().date()
                    start_dt = end_dt - timedelta(days=days + 30)  # å¤šå–30å¤©ç¡®ä¿æ•°æ®å……è¶³
                    price_start = start_dt.strftime('%Y-%m-%d')
                    price_end = end_dt.strftime('%Y-%m-%d')
                else:
                    # é»˜è®¤å–è¿‡å»3ä¸ªæœˆæ•°æ®
                    end_dt = datetime.now().date()
                    start_dt = end_dt - timedelta(days=90)
                    price_start = start_dt.strftime('%Y-%m-%d')
                    price_end = end_dt.strftime('%Y-%m-%d')
                
                # è·å–ä»·æ ¼æ•°æ®
                df = get_stock_data(symbol, price_start, price_end)
                
                if df is not None and not df.empty:
                    df['symbol'] = symbol
                    all_price_data.append(df)
                    
            except Exception as e:
                logger.warning(f"è·å– {symbol} ä»·æ ¼æ•°æ®å¤±è´¥: {e}")
                continue
        
        if all_price_data:
            combined_data = pd.concat(all_price_data, ignore_index=True)
            # ç¡®ä¿dateåˆ—æ˜¯datetimeç±»å‹
            combined_data['date'] = pd.to_datetime(combined_data['date'])
            return combined_data
        else:
            return pd.DataFrame()
            
    except Exception as e:
        logger.error(f"åŠ è½½ä»·æ ¼æ•°æ®å¤±è´¥: {e}")
        return pd.DataFrame()


class QueryRequest(BaseModel):
    query: str
    mode: str | None = None  # quick / llm / auto
    model: Optional[str] = None  # å¯é€‰ï¼šè¦†ç›–ä½¿ç”¨çš„LLMæ¨¡å‹ï¼ˆä¾‹å¦‚ "kimi"ï¼‰


class QueryResponse(BaseModel):
    result: str


def get_analyser(model_name: Optional[str] = None):
    selected_name = (model_name or os.getenv("MODEL_NAME", "deepseek-chat")).strip()
    llm = ChatModelFactory.get_model(selected_name)
    analyser = StockAnalyser(llm=llm, verbose=False)
    # è£…è½½å·¥å…·ï¼ˆä¾› llm æ¨¡å¼ä½¿ç”¨ï¼‰
    try:
        tools = StockTool.extract_tools_from_file("./data/stock.md.txt")
        analyser.set_tools(tools)
    except Exception:
        pass
    return analyser


@app.post("/api/analyse", response_model=QueryResponse)
def analyse(req: QueryRequest):
    analyser = get_analyser(req.model)
    result = analyser.analyse(req.query, mode=req.mode)
    # æ£€æµ‹æŸ¥è¯¢ä¸­çš„è‚¡ç¥¨ä»£ç å¹¶é™„åŠ å¤šç»´åº¦æŒ‡æ ‡
    try:
        extra = _append_quick_indicators_if_any(req.query)
        if extra:
            result = f"{result}\n\n{extra}"
    except Exception:
        pass
    # äºŒæ¬¡æ€»ç»“ï¼šè¾“å‡ºæ˜ç¡®çš„ç»“è®º/å»ºè®®/é£é™©/æ“ä½œ
    try:
        summary = _llm_summarize_conclusion(req.query, result, req.model)
        if summary:
            return QueryResponse(result=summary)
    except Exception:
        pass
    return QueryResponse(result=result)


history: list[dict] = []


@app.post("/api/search", response_model=QueryResponse)
def search(req: QueryRequest):
    analyser = get_analyser(req.model)
    result = analyser.analyse(req.query, mode=req.mode)
    try:
        extra = _append_quick_indicators_if_any(req.query)
        if extra:
            result = f"{result}\n\n{extra}"
    except Exception:
        pass
    try:
        summary = _llm_summarize_conclusion(req.query, result, req.model)
        if summary:
            history.append({"query": req.query, "mode": req.mode or "auto", "result": summary, "model": req.model or os.getenv("MODEL_NAME", "deepseek-chat")})
            return QueryResponse(result=summary)
    except Exception:
        pass
    history.append({"query": req.query, "mode": req.mode or "auto", "result": result, "model": req.model or os.getenv("MODEL_NAME", "deepseek-chat")})
    return QueryResponse(result=result)

def _extract_symbol_from_text(text: str) -> Optional[str]:
    import re
    m = re.search(r"(?<!\d)(\d{6})(?!\d)", str(text))
    return m.group(1) if m else None

def _append_quick_indicators_if_any(text: str) -> Optional[str]:
    symbol = _extract_symbol_from_text(text)
    if not symbol or ak is None or pd is None:
        return None
    # è·å–è¿‘60å¤©æ•°æ®ï¼Œè®¡ç®—å…³é”®æŠ€æœ¯æŒ‡æ ‡
    try:
        # å–è¿‘180æ—¥ï¼Œç¡®ä¿æœ‰è¶³å¤Ÿæ•°æ®è®¡ç®—60æ—¥ç­‰æŒ‡æ ‡
        from datetime import datetime, timedelta
        end_dt = datetime.today()
        start_dt = end_dt - timedelta(days=180)
        start_str = start_dt.strftime("%Y%m%d")
        end_str = end_dt.strftime("%Y%m%d")
        df = ak.stock_zh_a_hist(symbol=symbol, period="daily", start_date=start_str, end_date=end_str, adjust="")
        if df is None or df.empty:
            return None
        df = df.tail(120).copy()
        close = pd.to_numeric(df["æ”¶ç›˜"], errors="coerce")
        high = pd.to_numeric(df["æœ€é«˜"], errors="coerce")
        low = pd.to_numeric(df["æœ€ä½"], errors="coerce")
        vol = pd.to_numeric(df.get("æˆäº¤é‡"), errors="coerce") if "æˆäº¤é‡" in df.columns else None
        valid = close.notna() & high.notna() & low.notna()
        close = close[valid]
        high = high[valid]
        low = low[valid]
        if vol is not None:
            vol = vol[valid]
        if len(close) < 30:
            return None

        import numpy as _np

        ma20 = close.rolling(20, min_periods=20).mean()
        ma60 = close.rolling(60, min_periods=60).mean()
        ma20_slope = (ma20.iloc[-1] - ma20.iloc[-5]) / ma20.iloc[-5] * 100 if ma20.iloc[-5] and _np.isfinite(ma20.iloc[-5]) else _np.nan

        # ä»·é‡è¶‹åŠ¿ä¸åŠ¨é‡
        ret5 = (close.iloc[-1] / close.iloc[-6] - 1) * 100 if len(close) >= 6 and _np.isfinite(close.iloc[-6]) else _np.nan
        vol_ratio = None
        if vol is not None and len(vol) >= 60:
            v20 = vol.rolling(20, min_periods=20).mean().iloc[-1]
            v60 = vol.rolling(60, min_periods=60).mean().iloc[-1]
            if _np.isfinite(v20) and _np.isfinite(v60) and v60:
                vol_ratio = float(v20 / v60)

        # RSI14
        delta = close.diff()
        gain = (delta.clip(lower=0)).rolling(14, min_periods=14).mean()
        loss = (-delta.clip(upper=0)).rolling(14, min_periods=14).mean()
        rs = gain / (loss.replace(0, _np.nan))
        rsi14 = 100 - (100 / (1 + rs))
        rsi_val = float(rsi14.iloc[-1]) if _np.isfinite(rsi14.iloc[-1]) else None

        # MACD(12,26,9)
        ema12 = close.ewm(span=12, adjust=False).mean()
        ema26 = close.ewm(span=26, adjust=False).mean()
        dif = ema12 - ema26
        dea = dif.ewm(span=9, adjust=False).mean()
        macd_hist = (dif - dea).iloc[-1]

        # å¸ƒæ—å¸¦ä½ç½®
        mid = close.rolling(20, min_periods=20).mean()
        std = close.rolling(20, min_periods=20).std()
        upper = mid + 2 * std
        lower = mid - 2 * std
        bpos = None
        if _np.isfinite(upper.iloc[-1]) and _np.isfinite(lower.iloc[-1]):
            rng = (upper.iloc[-1] - lower.iloc[-1]) or _np.nan
            bpos = float((close.iloc[-1] - lower.iloc[-1]) / rng * 100) if _np.isfinite(rng) else None

        # ATR14 (% of price)
        prev_close = close.shift(1)
        tr = _np.maximum(high - low, _np.maximum((high - prev_close).abs(), (low - prev_close).abs()))
        atr = pd.Series(tr).rolling(14, min_periods=14).mean()
        atr_pct = float(atr.iloc[-1] / close.iloc[-1] * 100) if _np.isfinite(atr.iloc[-1]) else None

        # ç®€å•é£æ§ï¼šè¿‘60æ—¥æœ€å¤§å›æ’¤
        last60 = close.tail(60)
        mdd = None
        if len(last60) >= 2:
            roll_max = last60.cummax()
            drawdown = last60 / roll_max - 1.0
            mdd = float(drawdown.min() * 100)

        parts = []
        if _np.isfinite(ma20_slope):
            parts.append(f"MA20æ–œç‡: {ma20_slope:.2f}%")
        if ma60.iloc[-1] == ma60.iloc[-1]:
            parts.append(f"MA60: {ma60.iloc[-1]:.2f}")
        if _np.isfinite(ret5):
            parts.append(f"5æ—¥åŠ¨é‡: {ret5:.2f}%")
        if vol_ratio is not None:
            parts.append(f"é‡èƒ½æ¯”(20/60): {vol_ratio:.2f}")
        if rsi_val is not None:
            parts.append(f"RSI14: {rsi_val:.1f}")
        if macd_hist == macd_hist:
            parts.append(f"MACDæŸ±: {macd_hist:.3f}")
        if bpos is not None:
            parts.append(f"å¸ƒæ—å¸¦ä½ç½®: {bpos:.1f}%")
        if atr_pct is not None:
            parts.append(f"ATR14æ³¢åŠ¨ç‡: {atr_pct:.2f}%")
        if mdd is not None:
            parts.append(f"60æ—¥æœ€å¤§å›æ’¤: {mdd:.2f}%")

        if parts:
            return "ğŸ“Š é™„åŠ æŒ‡æ ‡æ¦‚è§ˆï¼ˆè¿‘120æ—¥ï¼‰\n- " + "\n- ".join(parts)
        return None
    except Exception:
        return None

def _llm_summarize_conclusion(query: str, analysed_text: str, model_name: Optional[str] = None) -> Optional[str]:
    """ç”¨æ‰€é€‰LLMå°†æ•°æ®ä¸æŒ‡æ ‡æ±‡æ€»ä¸ºæ˜ç¡®çš„ç»“è®º/å»ºè®®/é£é™©/æ“ä½œå››æ®µç»“æ„ã€‚"""
    try:
        llm = ChatModelFactory.get_model((model_name or os.getenv("MODEL_NAME", "deepseek-chat")).strip())
        symbol = _extract_symbol_from_text(query) or ""
        prompt = (
            "ä½ æ˜¯èµ„æ·±Aè‚¡äº¤æ˜“é¡¾é—®ã€‚è¯·åŸºäºä»¥ä¸‹â€˜æ•°æ®è¦ç‚¹ä¸æŒ‡æ ‡â€™è¾“å‡ºç®€æ´çš„ä¸­æ–‡ç»“è®ºï¼Œä¸¥æ ¼åŒ…å«å››ä¸ªéƒ¨åˆ†ï¼š\n"
            "1) ç»“è®ºï¼šä¸€å¥è¯åˆ¤å®šå½“ä¸‹è¶‹åŠ¿ä¸ç»“æ„æ€§ä½ç½®ï¼ˆçœ‹å¤š/è§‚æœ›/è°¨æ…ï¼‰ã€‚\n"
            "2) å»ºè®®ï¼š2-4æ¡å¯æ‰§è¡Œå»ºè®®ï¼ˆå«ä»“ä½/èŠ‚å¥/å…³æ³¨ä½ï¼‰ã€‚\n"
            "3) é£é™©ï¼š2-3æ¡ä¸»è¦é£é™©ä¸è§¦å‘æ¡ä»¶ã€‚\n"
            "4) æ“ä½œï¼šä»Šæ—¥/æœ¬å‘¨çš„æ“ä½œè®¡åˆ’ä¸è§¦å‘ä»·ä½ã€‚\n"
            "è¦æ±‚ï¼š\n- ç”¨äº‹å®æ”¯æ’‘ï¼Œä¸å¤è¿°è¡¨æ ¼ï¼›\n- ä¸è¦è¯·æ±‚æ›´å¤šä¿¡æ¯ï¼›\n- æ§åˆ¶åœ¨150-220å­—ï¼›\n- ç›´æ¥è¾“å‡ºå››æ®µï¼Œæ— éœ€å‰åç¼€ã€‚\n\n"
            f"æ ‡çš„: {symbol or 'æœªçŸ¥'}\nç”¨æˆ·é—®é¢˜: {query}\n\næ•°æ®è¦ç‚¹ä¸æŒ‡æ ‡ï¼ˆåŸå§‹è¾“å‡ºå¦‚ä¸‹ï¼‰:\n{analysed_text}\n"
        )
        resp = llm.invoke(prompt)
        content = getattr(resp, "content", str(resp)).strip()
        # ç®€å•æ ¡éªŒæ˜¯å¦å«æœ‰å…³é”®åˆ†æ®µè¯ï¼Œä¸æ»¡è¶³åˆ™è¿”å›Noneä»¥å›é€€åŸç»“æœ
        if any(k in content for k in ["ç»“è®º", "å»ºè®®", "é£é™©", "æ“ä½œ"]):
            return content
        return None
    except Exception:
        return None


@app.get("/api/history")
def get_history():
    return list(reversed(history))[:100]


class FixedAnalyseRequest(BaseModel):
    symbol: str
    analyses: List[str]  # ["MA20", "RSI14", "MACD", "BOLL", "REALTIME", "TOPUP", "TOPDOWN"]
    lookback_days: int | None = 120
    with_summary: bool = True


def _fetch_hist(symbol: str, start_date: str, end_date: str) -> Any:
    if ak is None or pd is None:
        raise HTTPException(status_code=500, detail="åç«¯ç¼ºå°‘ä¾èµ–ï¼šakshare/pandas")
    df = ak.stock_zh_a_hist(symbol=symbol, period="daily", start_date=start_date, end_date=end_date, adjust="")
    if df is None or df.empty:
        raise HTTPException(status_code=400, detail="æœªè·å–åˆ°å†å²æ•°æ®")
    return df


def _date_range(lookback_days: int) -> tuple[str, str]:
    from datetime import datetime, timedelta
    end_dt = datetime.today()
    start_dt = end_dt - timedelta(days=lookback_days)
    return start_dt.strftime("%Y%m%d"), end_dt.strftime("%Y%m%d")


@app.post("/api/fixed_analyse")
def fixed_analyse(req: FixedAnalyseRequest):
    symbol = req.symbol.lower().replace("sh", "").replace("sz", "")
    start_date, end_date = _date_range(req.lookback_days or 120)
    logs: list[str] = []
    result: Dict[str, Any] = {
        "meta": {"symbol": req.symbol, "start": start_date, "end": end_date, "analyses": req.analyses},
        "series": {},   # ç”¨äºå›¾è¡¨çš„åºåˆ—
        "indicators": {},
        "realtime": None,
        "board": None,
        "summary": None,
        "logs": logs,
    }

    try:
        df = None
        need_hist = any(k in req.analyses for k in ["MA20", "RSI14", "MACD", "BOLL"]) 
        if need_hist:
            logs.append(f"å¼€å§‹è·å–å†å²æ•°æ®: symbol={symbol}, range={start_date}~{end_date}")
            df = _fetch_hist(symbol, start_date, end_date)
            logs.append(f"å†å²æ•°æ®è·å–å®Œæˆ: {len(df)} è¡Œ")

        if df is not None:
            # åŸºç¡€closeåºåˆ—
            result["series"]["close"] = {
                "x": df["æ—¥æœŸ"].tolist(),
                "y": [float(v) for v in df["æ”¶ç›˜"].tolist()],
            }

        if "MA20" in req.analyses and df is not None:
            logs.append("è®¡ç®—MA20â€¦")
            ma = df["æ”¶ç›˜"].rolling(window=20).mean()
            result["series"]["MA20"] = {
                "x": df["æ—¥æœŸ"].tolist(),
                "y": [None if pd.isna(v) else float(v) for v in ma.tolist()],
            }

        if "RSI14" in req.analyses and df is not None:
            logs.append("è®¡ç®—RSI14â€¦")
            delta = df["æ”¶ç›˜"].diff()
            gain = delta.clip(lower=0).rolling(window=14).mean()
            loss = (-delta.clip(upper=0)).rolling(window=14).mean()
            rs = gain / loss.replace(0, pd.NA)
            rsi = 100 - (100 / (1 + rs))
            result["series"]["RSI14"] = {
                "x": df["æ—¥æœŸ"].tolist(),
                "y": [None if pd.isna(v) else float(v) for v in rsi.tolist()],
            }
            last = rsi.dropna().tail(1)
            result["indicators"]["RSI14"] = float(last.iloc[0]) if not last.empty else None

        if "MACD" in req.analyses and df is not None:
            logs.append("è®¡ç®—MACDâ€¦")
            ema12 = df["æ”¶ç›˜"].ewm(span=12).mean()
            ema26 = df["æ”¶ç›˜"].ewm(span=26).mean()
            macd = ema12 - ema26
            signal = macd.ewm(span=9).mean()
            hist = macd - signal
            result["series"]["MACD"] = {
                "x": df["æ—¥æœŸ"].tolist(),
                "y": [float(v) for v in macd.tolist()],
            }
            result["series"]["MACD_SIGNAL"] = {
                "x": df["æ—¥æœŸ"].tolist(),
                "y": [float(v) for v in signal.tolist()],
            }
            result["series"]["MACD_HIST"] = {
                "x": df["æ—¥æœŸ"].tolist(),
                "y": [float(v) for v in hist.tolist()],
            }

        if "BOLL" in req.analyses and df is not None:
            logs.append("è®¡ç®—å¸ƒæ—å¸¦(20,2)â€¦")
            ma20 = df["æ”¶ç›˜"].rolling(window=20).mean()
            std = df["æ”¶ç›˜"].rolling(window=20).std()
            upper = ma20 + 2 * std
            lower = ma20 - 2 * std
            result["series"]["BOLL_UP"] = {"x": df["æ—¥æœŸ"].tolist(), "y": [None if pd.isna(v) else float(v) for v in upper.tolist()]}
            result["series"]["BOLL_MID"] = {"x": df["æ—¥æœŸ"].tolist(), "y": [None if pd.isna(v) else float(v) for v in ma20.tolist()]}
            result["series"]["BOLL_LOW"] = {"x": df["æ—¥æœŸ"].tolist(), "y": [None if pd.isna(v) else float(v) for v in lower.tolist()]}

        if "REALTIME" in req.analyses:
            try:
                logs.append("è·å–å®æ—¶è¡Œæƒ…â€¦")
                spot = ak.stock_zh_a_spot_em()
                row = spot[spot["ä»£ç "].astype(str) == symbol]
                if row.empty:
                    row = spot[spot["ä»£ç "].astype(str).str.contains(symbol)]
                if not row.empty:
                    # æ¸…ç†NaNå€¼
                    realtime_data = row.head(1).fillna(None).to_dict(orient="records")[0]
                    result["realtime"] = realtime_data
                else:
                    result["realtime"] = None
            except Exception:
                result["realtime"] = None

        if "TOPUP" in req.analyses or "TOPDOWN" in req.analyses:
            try:
                logs.append("è·å–æ¶¨è·Œå¹…æ¦œâ€¦")
                board = ak.stock_zh_a_spot_em()
                board["æ¶¨è·Œå¹…"] = pd.to_numeric(board["æ¶¨è·Œå¹…"], errors="coerce")
                board["æœ€æ–°ä»·"] = pd.to_numeric(board["æœ€æ–°ä»·"], errors="coerce")
                if "TOPDOWN" in req.analyses:
                    board = board.sort_values("æ¶¨è·Œå¹…").head(10)
                else:
                    board = board.sort_values("æ¶¨è·Œå¹…", ascending=False).head(10)
                # æ¸…ç†NaNå€¼ï¼Œæ›¿æ¢ä¸ºNone
                board_clean = board[["ä»£ç ", "åç§°", "æœ€æ–°ä»·", "æ¶¨è·Œå¹…"]].fillna(None)
                result["board"] = board_clean.to_dict(orient="records")
            except Exception:
                result["board"] = None

        # LLM æ€»ç»“ï¼ˆå¯é€‰ï¼‰
        if req.with_summary:
            try:
                logs.append("è°ƒç”¨LLMç”Ÿæˆç»¼åˆç»“è®ºâ€¦")
                llm = ChatModelFactory.get_model(os.getenv("MODEL_NAME", "deepseek-chat"))
                # åŸºäºå·²é€‰æŒ‡æ ‡æ„é€ äº‹å®
                facts_lines: list[str] = []
                try:
                    # MA20
                    if "MA20" in req.analyses and df is not None and "MA20" in result["series"]:
                        latest_close = float(df["æ”¶ç›˜"].tail(1).iloc[0])
                        latest_ma = result["series"]["MA20"]["y"][-1]
                        rel = None
                        if latest_ma is not None:
                            rel = "ä¸Šæ–¹" if latest_close >= latest_ma else "ä¸‹æ–¹"
                        # ç®€æ˜“è¶‹åŠ¿
                        ma_vals = [v for v in result["series"]["MA20"]["y"] if v is not None]
                        trend = None
                        if len(ma_vals) >= 5:
                            import statistics
                            diff = ma_vals[-1] - ma_vals[-5]
                            trend = "ä¸Šå‡" if diff > 0 else ("ä¸‹é™" if diff < 0 else "éœ‡è¡")
                        facts_lines.append(f"MA20: æ”¶ç›˜ {latest_close:.2f}, MA20 {latest_ma if latest_ma is not None else 'NA'}, ä»·æ ¼ä½äºMA20{rel or 'æœªçŸ¥'}, è¶‹åŠ¿{trend or 'æœªçŸ¥'}")

                    # RSI14
                    if "RSI14" in req.analyses and "RSI14" in result["series"]:
                        rsi_last = result["series"]["RSI14"]["y"][-1]
                        state = None
                        if rsi_last is not None:
                            state = "è¶…ä¹°" if rsi_last >= 70 else ("è¶…å–" if rsi_last <= 30 else "ä¸­æ€§")
                        facts_lines.append(f"RSI14: {rsi_last if rsi_last is not None else 'NA'} ({state or 'æœªçŸ¥'})")

                    # MACD
                    if "MACD" in req.analyses and ("MACD" in result["series"] or "MACD_SIGNAL" in result["series"]):
                        macd_last = result["series"].get("MACD", {}).get("y", [None])[-1]
                        signal_last = result["series"].get("MACD_SIGNAL", {}).get("y", [None])[-1]
                        cross = None
                        if macd_last is not None and signal_last is not None:
                            cross = "é‡‘å‰" if macd_last >= signal_last else "æ­»å‰"
                        facts_lines.append(f"MACD: {macd_last if macd_last is not None else 'NA'}, Signal: {signal_last if signal_last is not None else 'NA'} ({cross or 'æœªçŸ¥'})")

                    # BOLL
                    if "BOLL" in req.analyses and all(k in result["series"] for k in ["BOLL_UP","BOLL_MID","BOLL_LOW"]) and df is not None:
                        up = result["series"]["BOLL_UP"]["y"][-1]
                        mid = result["series"]["BOLL_MID"]["y"][-1]
                        low = result["series"]["BOLL_LOW"]["y"][-1]
                        close_last = float(df["æ”¶ç›˜"].tail(1).iloc[0])
                        pos = None
                        if None not in [up, mid, low]:
                            if close_last >= up:
                                pos = "ä¸Šè½¨å¤–"
                            elif close_last >= mid:
                                pos = "ä¸Šè½¨-ä¸­è½¨"
                            elif close_last >= low:
                                pos = "ä¸­è½¨-ä¸‹è½¨"
                            else:
                                pos = "ä¸‹è½¨å¤–"
                        facts_lines.append(f"BOLL: ä¸Š {up if up is not None else 'NA'}, ä¸­ {mid if mid is not None else 'NA'}, ä¸‹ {low if low is not None else 'NA'}, ä»·æ ¼ä½ç½® {pos or 'æœªçŸ¥'}")

                    # å®æ—¶
                    if "REALTIME" in req.analyses and result["realtime"]:
                        rt = result["realtime"]
                        facts_lines.append(f"å®æ—¶: åç§° {rt.get('åç§°','-')}, ä»·æ ¼ {rt.get('æœ€æ–°ä»·','-')}, æ¶¨è·Œå¹… {rt.get('æ¶¨è·Œå¹…','-')}%")
                except Exception:
                    pass

                facts_text = "\n".join([f"- {line}" for line in facts_lines]) if facts_lines else "- æ— "

                # ç®€è¦æç¤ºï¼Œè¦æ±‚åŸºäºæ‰€é€‰æŒ‡æ ‡ç»™å‡ºæ€»ç»“
                prompt = (
                    "ä½ æ˜¯ä¸€åä¸“ä¸šè¯åˆ¸åˆ†æå¸ˆã€‚è¯·ä»…åŸºäºæˆ‘æä¾›çš„æŒ‡æ ‡äº‹å®è¿›è¡Œä¸­æ–‡æ€»ç»“ï¼Œçº¦300å­—ï¼ŒåŒ…å«ï¼šè¶‹åŠ¿åˆ¤æ–­ã€å…³é”®ä¿¡å·ã€é£é™©ä¸æ³¨æ„äº‹é¡¹ã€è‹¥å¹²å¯æ‰§è¡Œçš„è§‚å¯Ÿç‚¹ã€‚ä¸è¦æœæ’°æœªæä¾›çš„æ•°æ®ã€‚\n"
                    f"æ ‡çš„: {req.symbol}\n"
                    f"åŒºé—´: {start_date}~{end_date}\n"
                    f"æ‰€é€‰æŒ‡æ ‡: {', '.join(req.analyses)}\n"
                    "æŒ‡æ ‡äº‹å®å¦‚ä¸‹:\n" + facts_text
                )
                summary = llm.invoke(prompt)
                # å…¼å®¹ä¸åŒè¿”å›ç±»å‹
                result["summary"] = getattr(summary, "content", str(summary))
                logs.append("ç»¼åˆç»“è®ºç”Ÿæˆå®Œæˆ")
            except Exception:
                result["summary"] = None
                logs.append("ç»¼åˆç»“è®ºç”Ÿæˆå¤±è´¥ï¼Œå·²å¿½ç•¥")

        return custom_json_response(result)
    except HTTPException:
        raise
    except Exception:
        traceback.print_exc()
        raise HTTPException(status_code=500, detail="å›ºå®šåˆ†æå¤±è´¥")


def _sse_format(event: str, data: str) -> str:
    return f"event: {event}\ndata: {data}\n\n"


def _generate_default_analysis(analyses: list, result: dict, df) -> dict:
    """åŸºäºæŠ€æœ¯æŒ‡æ ‡ç”Ÿæˆé»˜è®¤çš„ç»“æ„åŒ–åˆ†æ"""
    signals = []
    risk_points = []
    confidence_scores = []
    support_level = None
    resistance_level = None
    
    # åˆ¤æ–­è¶‹åŠ¿ä¿¡å·
    trend_signal = "éœ‡è¡"  # é»˜è®¤
    
    if df is not None:
        latest_close = float(df["æ”¶ç›˜"].tail(1).iloc[0])
        
        # MAåˆ†æ
        ma_signals = []
        for ma_period in ["MA5", "MA20", "MA60"]:
            if ma_period in analyses and ma_period in result["series"]:
                ma_values = [v for v in result["series"][ma_period]["y"] if v is not None]
                if ma_values:
                    latest_ma = ma_values[-1]
                    if latest_close > latest_ma:
                        ma_signals.append(f"{ma_period}ä¸Šæ–¹")
                        confidence_scores.append(0.6)
                    else:
                        ma_signals.append(f"{ma_period}ä¸‹æ–¹")
                        confidence_scores.append(0.4)
                    
                    # è®¾å®šæ”¯æ’‘é˜»åŠ›ä½
                    if ma_period == "MA20":
                        if latest_close > latest_ma:
                            support_level = round(latest_ma, 2)
                        else:
                            resistance_level = round(latest_ma, 2)
        
        # RSIåˆ†æ
        if "RSI14" in analyses and "RSI14" in result["series"]:
            rsi_values = [v for v in result["series"]["RSI14"]["y"] if v is not None]
            if rsi_values:
                latest_rsi = rsi_values[-1]
                if latest_rsi >= 70:
                    signals.append("RSIè¶…ä¹°ä¿¡å·")
                    risk_points.append("RSIè¶…ä¹°ï¼Œæ³¨æ„å›è°ƒé£é™©")
                    confidence_scores.append(0.3)
                elif latest_rsi <= 30:
                    signals.append("RSIè¶…å–ä¿¡å·")
                    signals.append("å¯èƒ½å‡ºç°åå¼¹")
                    confidence_scores.append(0.7)
                else:
                    signals.append("RSIä¸­æ€§åŒºé—´")
                    confidence_scores.append(0.5)
        
        # MACDåˆ†æ
        if "MACD" in analyses and "MACD" in result["series"] and "MACD_SIGNAL" in result["series"]:
            macd_values = [v for v in result["series"]["MACD"]["y"] if v is not None]
            signal_values = [v for v in result["series"]["MACD_SIGNAL"]["y"] if v is not None]
            if macd_values and signal_values:
                latest_macd = macd_values[-1]
                latest_signal = signal_values[-1]
                if latest_macd > latest_signal:
                    signals.append("MACDé‡‘å‰")
                    confidence_scores.append(0.6)
                else:
                    signals.append("MACDæ­»å‰")
                    confidence_scores.append(0.4)
        
        # ç»¼åˆåˆ¤æ–­è¶‹åŠ¿
        bullish_signals = sum(1 for s in ma_signals if "ä¸Šæ–¹" in s)
        total_ma_signals = len(ma_signals)
        
        if total_ma_signals > 0:
            bullish_ratio = bullish_signals / total_ma_signals
            if bullish_ratio >= 0.6:
                trend_signal = "å¤šå¤´"
            elif bullish_ratio <= 0.4:
                trend_signal = "ç©ºå¤´"
    
    # ç”Ÿæˆé£é™©æç¤º
    if not risk_points:
        if trend_signal == "å¤šå¤´":
            risk_points.append("æ³¨æ„å›è°ƒé£é™©ï¼Œè®¾ç½®æ­¢æŸ")
        elif trend_signal == "ç©ºå¤´":
            risk_points.append("ä¸‹è·Œé£é™©è¾ƒå¤§ï¼Œè°¨æ…æ“ä½œ")
        else:
            risk_points.append("æ–¹å‘ä¸æ˜ï¼Œç­‰å¾…æ˜ç¡®ä¿¡å·")
    
    # è®¡ç®—ç½®ä¿¡åº¦
    if confidence_scores:
        confidence = sum(confidence_scores) / len(confidence_scores)
    else:
        confidence = 0.5
    
    # æ„å»ºç»“æ„åŒ–æ•°æ®
    structured_data = {
        "trend_signal": trend_signal,
        "confidence": round(confidence, 2),
        "key_signals": signals[:3] if signals else ["æš‚æ— æ˜ç¡®ä¿¡å·"],  # æœ€å¤šæ˜¾ç¤º3ä¸ª
        "risk_points": risk_points[:2] if risk_points else ["è¯·æ³¨æ„é£é™©æ§åˆ¶"],  # æœ€å¤šæ˜¾ç¤º2ä¸ª
        "summary": f"åŸºäº{', '.join(analyses)}æŒ‡æ ‡åˆ†æï¼Œå½“å‰è¶‹åŠ¿ä¸º{trend_signal}ï¼Œç½®ä¿¡åº¦{round(confidence*100, 1)}%"
    }
    
    if support_level:
        structured_data["support_level"] = support_level
    if resistance_level:
        structured_data["resistance_level"] = resistance_level
    
    return structured_data


def _predict_price_direction(df, result: dict, analyses: list) -> dict:
    """åŸºäºæŠ€æœ¯æŒ‡æ ‡å’Œå¢å¼ºæ—¶é—´åºåˆ—æ¨¡å‹é¢„æµ‹æœªæ¥5-10æ—¥ä»·æ ¼æ–¹å‘"""
    if df is None or len(df) < 30:  # å¢åŠ æœ€å°æ•°æ®è¦æ±‚
        return {"error": "æ•°æ®ä¸è¶³ï¼Œæ— æ³•é¢„æµ‹ï¼ˆè‡³å°‘éœ€è¦30ä¸ªäº¤æ˜“æ—¥ï¼‰"}
    
    try:
        import numpy as np
        import math
        
        # å‡†å¤‡ç‰¹å¾æ•°æ®
        features = []
        feature_names = []
        prices = df["æ”¶ç›˜"].values
        volumes = df["æˆäº¤é‡"].values if "æˆäº¤é‡" in df.columns else None
        
        # === å¢å¼ºä»·æ ¼åŠ¨é‡ç‰¹å¾ ===
        price_changes = np.diff(prices) / prices[:-1]  # ä½¿ç”¨ç™¾åˆ†æ¯”å˜åŒ–
        
        # å¤šæ—¶é—´ç»´åº¦åŠ¨é‡
        if len(price_changes) >= 20:
            momentum_3d = np.mean(price_changes[-3:])  # 3æ—¥åŠ¨é‡
            momentum_5d = np.mean(price_changes[-5:])  # 5æ—¥åŠ¨é‡
            momentum_10d = np.mean(price_changes[-10:])  # 10æ—¥åŠ¨é‡
            momentum_20d = np.mean(price_changes[-20:])  # 20æ—¥åŠ¨é‡
            
            features.extend([momentum_3d, momentum_5d, momentum_10d, momentum_20d])
            feature_names.extend(['momentum_3d', 'momentum_5d', 'momentum_10d', 'momentum_20d'])
        else:
            features.extend([0, 0, 0, 0])
            feature_names.extend(['momentum_3d', 'momentum_5d', 'momentum_10d', 'momentum_20d'])
        
        # === æ³¢åŠ¨ç‡ç‰¹å¾ ===
        if len(price_changes) >= 10:
            vol_short = np.std(price_changes[-5:])   # çŸ­æœŸæ³¢åŠ¨ç‡
            vol_medium = np.std(price_changes[-10:]) # ä¸­æœŸæ³¢åŠ¨ç‡
            vol_ratio = vol_short / vol_medium if vol_medium > 0 else 1  # æ³¢åŠ¨ç‡æ¯”å€¼
            
            features.extend([vol_short, vol_ratio])
            feature_names.extend(['volatility_short', 'volatility_ratio'])
        else:
            features.extend([0, 1])
            feature_names.extend(['volatility_short', 'volatility_ratio'])
        
        # === å‡çº¿æ’åˆ—ç‰¹å¾ ===
        ma_alignment_score = 0
        ma_trend_strength = 0
        
        # æ£€æŸ¥MA5, MA20, MA60çš„æ’åˆ—
        ma_values = {}
        for ma_period in ["MA5", "MA20", "MA60"]:
            if ma_period in analyses and ma_period in result["series"]:
                ma_vals = [v for v in result["series"][ma_period]["y"] if v is not None]
                if len(ma_vals) >= 3:
                    ma_values[ma_period] = ma_vals[-3:]  # å–æœ€è¿‘3ä¸ªå€¼
        
        if len(ma_values) >= 2:
            current_price = prices[-1]
            
            # å‡çº¿å¤šå¤´æ’åˆ—å¾—åˆ†
            if "MA5" in ma_values and "MA20" in ma_values:
                if ma_values["MA5"][-1] > ma_values["MA20"][-1] > current_price * 0.95:
                    ma_alignment_score += 0.5
                elif ma_values["MA5"][-1] < ma_values["MA20"][-1] < current_price * 1.05:
                    ma_alignment_score -= 0.5
                    
            # å‡çº¿è¶‹åŠ¿å¼ºåº¦ï¼ˆæ–œç‡ä¸€è‡´æ€§ï¼‰
            for ma_period, vals in ma_values.items():
                if len(vals) >= 3:
                    slope1 = (vals[-1] - vals[-2]) / vals[-2] if vals[-2] != 0 else 0
                    slope2 = (vals[-2] - vals[-3]) / vals[-3] if vals[-3] != 0 else 0
                    if slope1 * slope2 > 0:  # åŒå‘
                        ma_trend_strength += abs(slope1) * 0.3
        
        features.extend([ma_alignment_score, ma_trend_strength])
        feature_names.extend(['ma_alignment', 'ma_trend_strength'])
        
        # === RSIå¢å¼ºç‰¹å¾ ===
        rsi_score = 0
        rsi_divergence = 0
        
        if "RSI14" in analyses and "RSI14" in result["series"]:
            rsi_values = [v for v in result["series"]["RSI14"]["y"] if v is not None]
            if len(rsi_values) >= 5:
                latest_rsi = rsi_values[-1]
                
                # RSIä½ç½®å¾—åˆ†ï¼ˆéçº¿æ€§ï¼‰
                if latest_rsi > 70:
                    rsi_score = -((latest_rsi - 70) / 30) ** 2  # è¶…ä¹°æƒ©ç½š
                elif latest_rsi < 30:
                    rsi_score = ((30 - latest_rsi) / 30) ** 2   # è¶…å–å¥–åŠ±
                else:
                    rsi_score = (latest_rsi - 50) / 50 * 0.5    # ä¸­æ€§åŒºåŸŸ
                
                # RSIèƒŒç¦»æ£€æµ‹
                if len(rsi_values) >= 5 and len(prices) >= 5:
                    price_trend = (prices[-1] - prices[-5]) / prices[-5]
                    rsi_trend = (rsi_values[-1] - rsi_values[-5]) / 100
                    
                    # ä»·æ ¼ä¸Šæ¶¨ä½†RSIä¸‹é™ï¼ˆé¡¶èƒŒç¦»ï¼‰
                    if price_trend > 0.02 and rsi_trend < -0.05:
                        rsi_divergence = -0.3
                    # ä»·æ ¼ä¸‹è·Œä½†RSIä¸Šå‡ï¼ˆåº•èƒŒç¦»ï¼‰
                    elif price_trend < -0.02 and rsi_trend > 0.05:
                        rsi_divergence = 0.3
        
        features.extend([rsi_score, rsi_divergence])
        feature_names.extend(['rsi_score', 'rsi_divergence'])
        
        # === MACDå¢å¼ºç‰¹å¾ ===
        macd_signal = 0
        macd_histogram_trend = 0
        
        if "MACD" in analyses and "MACD" in result["series"]:
            macd_values = [v for v in result["series"]["MACD"]["y"] if v is not None]
            signal_values = [v for v in result["series"]["MACD_SIGNAL"]["y"] if v is not None]
            
            if len(macd_values) >= 3 and len(signal_values) >= 3:
                # MACDé‡‘å‰æ­»å‰
                current_diff = macd_values[-1] - signal_values[-1]
                prev_diff = macd_values[-2] - signal_values[-2]
                
                if current_diff > 0 and prev_diff <= 0:  # é‡‘å‰
                    macd_signal = 0.4
                elif current_diff < 0 and prev_diff >= 0:  # æ­»å‰
                    macd_signal = -0.4
                else:
                    macd_signal = current_diff * 0.1  # æŒç»­ä¿¡å·
                
                # MACDæŸ±çŠ¶å›¾è¶‹åŠ¿
                if len(macd_values) >= 3:
                    hist_trend = (current_diff - prev_diff)
                    macd_histogram_trend = hist_trend * 0.2
        
        features.extend([macd_signal, macd_histogram_trend])
        feature_names.extend(['macd_signal', 'macd_histogram'])
        
        # === æˆäº¤é‡ç¡®è®¤ç‰¹å¾ ===
        volume_confirmation = 0
        if volumes is not None and len(volumes) >= 10:
            vol_avg = np.mean(volumes[-10:])
            vol_recent = np.mean(volumes[-3:])
            
            if vol_recent > vol_avg * 1.2:  # æ”¾é‡
                price_trend_3d = (prices[-1] - prices[-4]) / prices[-4] if len(prices) >= 4 else 0
                volume_confirmation = 0.2 if price_trend_3d > 0 else -0.1  # æ”¾é‡ä¸Šæ¶¨å¥½ï¼Œæ”¾é‡ä¸‹è·Œç•¥å
            elif vol_recent < vol_avg * 0.8:  # ç¼©é‡
                volume_confirmation = -0.1  # ç¼©é‡ä¸åˆ©
        
        features.append(volume_confirmation)
        feature_names.append('volume_confirmation')
        
        # === åŠ¨æ€æƒé‡åˆ†é… ===
        # æ ¹æ®å¸‚åœºç¯å¢ƒè°ƒæ•´æƒé‡
        volatility_level = np.std(price_changes[-10:]) if len(price_changes) >= 10 else 0.02
        
        # ğŸš€ ç»Ÿä¸€ä½¿ç”¨è¶‹åŠ¿è·Ÿè¸ªæƒé‡é…ç½®ï¼Œä¸“é—¨é’ˆå¯¹ç”¨æˆ·çš„è¶‹åŠ¿è‚¡
        weights = [0.25, 0.15, 0.10, 0.15,  # è¶‹åŠ¿å»¶ç»­65%
                  0.05, 0.05,               # æ³¢åŠ¨æ§åˆ¶10%  
                  0.15, 0.10,               # ä»·æ ¼ä½ç½®25%
                  0.0, 0.0,                 # RSI=0 (ç§»é™¤åè½¬é€»è¾‘)
                  0.0, 0.0,                 # MACD=0 
                  0.0]                      # æˆäº¤é‡=0
        
        # ç¡®ä¿ç‰¹å¾å’Œæƒé‡æ•°é‡åŒ¹é…
        if len(features) != len(weights):
            min_len = min(len(features), len(weights))
            features = features[:min_len]
            weights = weights[:min_len]
        
        # è®¡ç®—åŠ æƒå¾—åˆ†
        score = sum(f * w for f, w in zip(features, weights))
        
        # === ä¼˜åŒ–çš„æ¦‚ç‡è½¬æ¢ ===
        # ä½¿ç”¨æ ¡å‡†è¿‡çš„sigmoidå‡½æ•°
        sigmoid_factor = 3  # å¤§å¹…é™ä½æ”¾å¤§å€æ•°ï¼Œä¸å†å²æ•°æ®ç”Ÿæˆä¿æŒä¸€è‡´
        raw_probability = 1 / (1 + math.exp(-score * sigmoid_factor))
        
        # ğŸš€ ç§»é™¤æ‰€æœ‰æ ¡å‡†å¹²æ‰°ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹é¢„æµ‹
        calibrated_probability = raw_probability
        
        # === å¢å¼ºçš„ä»·æ ¼åŒºé—´é¢„æµ‹ ===
        latest_price = float(prices[-1])
        
        # ä½¿ç”¨å¤šç§æ³¢åŠ¨ç‡ä¼°è®¡
        vol_5d = np.std(price_changes[-5:]) if len(price_changes) >= 5 else 0.02
        vol_20d = np.std(price_changes[-20:]) if len(price_changes) >= 20 else 0.02
        vol_adaptive = vol_5d * 0.6 + vol_20d * 0.4  # è‡ªé€‚åº”æ³¢åŠ¨ç‡
        
        # åŸºäºæ¦‚ç‡å’Œæ³¢åŠ¨ç‡çš„ä»·æ ¼åŒºé—´
        expected_return = (calibrated_probability - 0.5) * 0.08  # æœ€å¤§æœŸæœ›æ¶¨è·Œå¹…8%
        
        # ä»·æ ¼åŒºé—´è®¡ç®—ï¼ˆè€ƒè™‘ååº¦ï¼‰
        upside_vol = vol_adaptive * 1.2 if calibrated_probability > 0.6 else vol_adaptive
        downside_vol = vol_adaptive * 1.2 if calibrated_probability < 0.4 else vol_adaptive
        
        price_lower = latest_price * (1 + expected_return - downside_vol * 1.5)
        price_upper = latest_price * (1 + expected_return + upside_vol * 1.5)
        
        # === åŠ¨æ€ç½®ä¿¡åº¦è®¡ç®— ===
        # åŸºäºç‰¹å¾ä¸€è‡´æ€§å’Œå¸‚åœºç¯å¢ƒ
        feature_consistency = abs(score) * 2  # ç‰¹å¾ä¸€è‡´æ€§
        market_stability = 1 / (1 + volatility_level * 50)  # å¸‚åœºç¨³å®šæ€§
        data_quality = min(1.0, len(prices) / 60)  # æ•°æ®è´¨é‡
        
        base_confidence = 50
        confidence_boost = (feature_consistency * 25 + market_stability * 20 + data_quality * 10)
        final_confidence = min(90, max(35, base_confidence + confidence_boost))
        
        # === æ„å»ºé¢„æµ‹ç»“æœ ===
        prediction = {
            "current_price": round(latest_price, 2),
            "prediction_days": "5-10æ—¥",
            "up_probability": round(calibrated_probability * 100, 1),
            "down_probability": round((1 - calibrated_probability) * 100, 1),
            "price_range_lower": round(max(price_lower, latest_price * 0.82), 2),  # æœ€å¤§è·Œå¹…18%
            "price_range_upper": round(min(price_upper, latest_price * 1.18), 2),  # æœ€å¤§æ¶¨å¹…18%
            "confidence_level": round(final_confidence, 1),
            "key_factors": [],
            "model_details": {  # æ–°å¢ï¼šæ¨¡å‹è¯¦æƒ…
                "feature_score": round(score, 3),
                "volatility_regime": "é«˜" if volatility_level > 0.03 else "ä½" if volatility_level < 0.015 else "ä¸­",
                "signal_strength": "å¼º" if abs(score) > 0.2 else "å¼±" if abs(score) < 0.05 else "ä¸­"
            }
        }
        
        # === æ™ºèƒ½å› ç´ åˆ†æ ===
        key_factors = []
        
        # ä¸»è¦è¶‹åŠ¿åˆ¤æ–­
        if calibrated_probability > 0.65:
            key_factors.append("å¤šå¤´è¶‹åŠ¿æ˜æ˜¾")
        elif calibrated_probability < 0.35:
            key_factors.append("ç©ºå¤´è¶‹åŠ¿æ˜æ˜¾") 
        else:
            key_factors.append("æ¨ªç›˜æ•´ç†æ€åŠ¿")
            
        # åŠ¨é‡åˆ†æ
        if len(features) >= 4:  # ç¡®ä¿æœ‰åŠ¨é‡ç‰¹å¾
            avg_momentum = np.mean(features[:4])
            if avg_momentum > 0.01:
                key_factors.append("ä»·æ ¼åŠ¨é‡å‘ä¸Š")
            elif avg_momentum < -0.01:
                key_factors.append("ä»·æ ¼åŠ¨é‡å‘ä¸‹")
                
        # æ³¢åŠ¨ç‡åˆ†æ
        if volatility_level > 0.03:
            key_factors.append("é«˜æ³¢åŠ¨ç¯å¢ƒ")
        elif volatility_level < 0.015:
            key_factors.append("ä½æ³¢åŠ¨ç¯å¢ƒ")
            
        # RSIçŠ¶æ€
        if "RSI14" in analyses and "RSI14" in result["series"]:
            rsi_values = [v for v in result["series"]["RSI14"]["y"] if v is not None]
            if rsi_values:
                latest_rsi = rsi_values[-1]
                if latest_rsi >= 75:
                    key_factors.append("RSIä¸¥é‡è¶…ä¹°")
                elif latest_rsi >= 65:
                    key_factors.append("RSIè½»åº¦è¶…ä¹°")
                elif latest_rsi <= 25:
                    key_factors.append("RSIä¸¥é‡è¶…å–")
                elif latest_rsi <= 35:
                    key_factors.append("RSIè½»åº¦è¶…å–")
                    
        # MACDçŠ¶æ€
        if macd_signal > 0.2:
            key_factors.append("MACDé‡‘å‰ä¿¡å·")
        elif macd_signal < -0.2:
            key_factors.append("MACDæ­»å‰ä¿¡å·")
            
        # æˆäº¤é‡çŠ¶æ€
        if volume_confirmation > 0.1:
            key_factors.append("æ”¾é‡ä¸Šæ¶¨ç¡®è®¤")
        elif volume_confirmation < -0.05:
            key_factors.append("é‡ä»·èƒŒç¦»é£é™©")
            
        prediction["key_factors"] = key_factors[:5]  # æœ€å¤šæ˜¾ç¤º5ä¸ªå› ç´ 
        
        # === ä¿å­˜é¢„æµ‹è®°å½•ç”¨äºåç»­æ ¡å‡† ===
        try:
            record = PredictionRecord(
                symbol=df.iloc[0].get("è‚¡ç¥¨ä»£ç ", "unknown") if "è‚¡ç¥¨ä»£ç " in df.columns else "unknown",
                prediction_date=datetime.now().strftime("%Y-%m-%d"),
                predicted_probability=calibrated_probability,
                prediction_horizon=5,
                features={
                    "feature_score": score,
                    "volatility_regime": "é«˜" if volatility_level > 0.03 else "ä½" if volatility_level < 0.015 else "ä¸­",
                    "signal_strength": abs(score)
                }
            )
            calibrator.save_prediction(record)
        except Exception as e:
            logger.warning(f"ä¿å­˜é¢„æµ‹è®°å½•å¤±è´¥: {e}")
        
        return prediction
        
    except Exception as e:
        return {"error": f"é¢„æµ‹è®¡ç®—å¤±è´¥: {str(e)}"}


@app.get("/api/fixed_analyse_stream")
def fixed_analyse_stream(symbol: str, analyses: str, lookback_days: int = 120, with_summary: bool = True):
    """
    SSEæµå¼è¾“å‡ºï¼š
      - event: log, data: æ–‡æœ¬æ—¥å¿—
      - event: done, data: æœ€ç»ˆJSONï¼ˆä¸ /api/fixed_analyse è¿”å›ç»“æ„ä¸€è‡´ï¼‰
    analyses: é€—å·åˆ†éš”ï¼Œä¾‹å¦‚ "MA20,RSI14,MACD"
    """
    analyses_list = [a.strip() for a in analyses.split(',') if a.strip()]

    def gen():
        try:
            yield _sse_format("log", "å¼€å§‹å›ºå®šåˆ†æ")
            # ç»„è£…è¯·æ±‚ä½“å¹¶è°ƒç”¨ç°æœ‰é€»è¾‘ï¼ˆå¤ç”¨å‡½æ•°ä½“ä»¥é¿å…é‡å¤ï¼‰
            # ç»Ÿä¸€å¤„ç†è‚¡ç¥¨ä»£ç æ ¼å¼ï¼Œä¸éæµå¼æ¥å£ä¿æŒä¸€è‡´
            processed_symbol = symbol.lower().replace("sh", "").replace("sz", "")
            req = FixedAnalyseRequest(symbol=processed_symbol, analyses=analyses_list, lookback_days=lookback_days, with_summary=with_summary)
            # æˆ‘ä»¬å¤åˆ¶ fixed_analyse çš„æµç¨‹ï¼Œä½†æŒ‰æ­¥yieldæ—¥å¿—
            logs: list[str] = []
            start_date, end_date = _date_range(req.lookback_days or 120)
            result: Dict[str, Any] = {
                "meta": {"symbol": req.symbol, "start": start_date, "end": end_date, "analyses": req.analyses},
                "series": {},
                "indicators": {},
                "realtime": None,
                "board": None,
                "summary": None,
                "logs": logs,
            }
            df = None
            need_hist = any(k in req.analyses for k in ["MA5", "MA20", "MA60", "RSI14", "MACD", "BOLL", "OBV", "ATR"])
            if need_hist:
                # è‡ªåŠ¨è°ƒæ•´æ—¶é—´èŒƒå›´ä»¥ç¡®ä¿MAçº¿æœ‰è¶³å¤Ÿçš„æœ‰æ•ˆæ•°æ®ç‚¹
                adjusted_lookback = req.lookback_days or 120
                if "MA60" in req.analyses:
                    adjusted_lookback = max(adjusted_lookback, 80)  # è‡³å°‘80å¤©ç¡®ä¿MA60æœ‰20ä¸ªæœ‰æ•ˆç‚¹
                elif "MA20" in req.analyses:
                    adjusted_lookback = max(adjusted_lookback, 50)  # è‡³å°‘50å¤©ç¡®ä¿MA20æœ‰30ä¸ªæœ‰æ•ˆç‚¹
                elif "MA5" in req.analyses:
                    adjusted_lookback = max(adjusted_lookback, 30)  # è‡³å°‘30å¤©ç¡®ä¿MA5æœ‰25ä¸ªæœ‰æ•ˆç‚¹
                
                if adjusted_lookback != (req.lookback_days or 120):
                    yield _sse_format("log", f"ä¸ºç¡®ä¿å‡çº¿æ•°æ®å®Œæ•´ï¼Œè‡ªåŠ¨è°ƒæ•´æŸ¥è¯¢æœŸé—´ä¸º{adjusted_lookback}å¤©")
                    adjusted_start, _ = _date_range(adjusted_lookback)
                    start_date = adjusted_start
                
                msg = f"å¼€å§‹è·å–å†å²æ•°æ®: symbol={processed_symbol}, range={start_date}~{end_date}"
                logs.append(msg)
                yield _sse_format("log", msg)
                df = _fetch_hist(processed_symbol, start_date, end_date)
                msg = f"å†å²æ•°æ®è·å–å®Œæˆ: {len(df)} è¡Œ"
                logs.append(msg)
                yield _sse_format("log", msg)
                # åŸºç¡€close
                result["series"]["close"] = {"x": df["æ—¥æœŸ"].tolist(), "y": [float(v) for v in df["æ”¶ç›˜"].tolist()]}

            # MAæŒ‡æ ‡ç³»åˆ—
            if "MA5" in req.analyses and df is not None:
                logs.append("è®¡ç®—MA5â€¦")
                yield _sse_format("log", "è®¡ç®—MA5â€¦")
                ma5 = df["æ”¶ç›˜"].rolling(window=5).mean()
                result["series"]["MA5"] = {"x": df["æ—¥æœŸ"].tolist(), "y": [None if pd.isna(v) else float(v) for v in ma5.tolist()]}

            if "MA20" in req.analyses and df is not None:
                logs.append("è®¡ç®—MA20â€¦")
                yield _sse_format("log", "è®¡ç®—MA20â€¦")
                ma20 = df["æ”¶ç›˜"].rolling(window=20).mean()
                result["series"]["MA20"] = {"x": df["æ—¥æœŸ"].tolist(), "y": [None if pd.isna(v) else float(v) for v in ma20.tolist()]}

            if "MA60" in req.analyses and df is not None:
                logs.append("è®¡ç®—MA60â€¦")
                yield _sse_format("log", "è®¡ç®—MA60â€¦")
                ma60 = df["æ”¶ç›˜"].rolling(window=60).mean()
                result["series"]["MA60"] = {"x": df["æ—¥æœŸ"].tolist(), "y": [None if pd.isna(v) else float(v) for v in ma60.tolist()]}

            if "RSI14" in req.analyses and df is not None:
                logs.append("è®¡ç®—RSI14â€¦")
                yield _sse_format("log", "è®¡ç®—RSI14â€¦")
                delta = df["æ”¶ç›˜"].diff()
                gain = delta.clip(lower=0).rolling(window=14).mean()
                loss = (-delta.clip(upper=0)).rolling(window=14).mean()
                rs = gain / loss.replace(0, pd.NA)
                rsi = 100 - (100 / (1 + rs))
                result["series"]["RSI14"] = {"x": df["æ—¥æœŸ"].tolist(), "y": [None if pd.isna(v) else float(v) for v in rsi.tolist()]}
                last = rsi.dropna().tail(1)
                result["indicators"]["RSI14"] = float(last.iloc[0]) if not last.empty else None

            if "MACD" in req.analyses and df is not None:
                logs.append("è®¡ç®—MACDâ€¦")
                yield _sse_format("log", "è®¡ç®—MACDâ€¦")
                ema12 = df["æ”¶ç›˜"].ewm(span=12).mean()
                ema26 = df["æ”¶ç›˜"].ewm(span=26).mean()
                macd = ema12 - ema26
                signal = macd.ewm(span=9).mean()
                hist = macd - signal
                result["series"]["MACD"] = {"x": df["æ—¥æœŸ"].tolist(), "y": [float(v) for v in macd.tolist()]}
                result["series"]["MACD_SIGNAL"] = {"x": df["æ—¥æœŸ"].tolist(), "y": [float(v) for v in signal.tolist()]}
                result["series"]["MACD_HIST"] = {"x": df["æ—¥æœŸ"].tolist(), "y": [float(v) for v in hist.tolist()]}

            if "BOLL" in req.analyses and df is not None:
                logs.append("è®¡ç®—å¸ƒæ—å¸¦(20,2)â€¦")
                yield _sse_format("log", "è®¡ç®—å¸ƒæ—å¸¦(20,2)â€¦")
                ma20 = df["æ”¶ç›˜"].rolling(window=20).mean()
                std = df["æ”¶ç›˜"].rolling(window=20).std()
                upper = ma20 + 2 * std
                lower = ma20 - 2 * std
                result["series"]["BOLL_UP"] = {"x": df["æ—¥æœŸ"].tolist(), "y": [None if pd.isna(v) else float(v) for v in upper.tolist()]}
                result["series"]["BOLL_MID"] = {"x": df["æ—¥æœŸ"].tolist(), "y": [None if pd.isna(v) else float(v) for v in ma20.tolist()]}
                result["series"]["BOLL_LOW"] = {"x": df["æ—¥æœŸ"].tolist(), "y": [None if pd.isna(v) else float(v) for v in lower.tolist()]}

            if "OBV" in req.analyses and df is not None:
                logs.append("è®¡ç®—OBVèƒ½é‡æ½®â€¦")
                yield _sse_format("log", "è®¡ç®—OBVèƒ½é‡æ½®â€¦")
                # OBV = æˆäº¤é‡ * sign(æ”¶ç›˜-æ˜¨æ”¶)çš„ç´¯ç§¯
                price_change = df["æ”¶ç›˜"].diff()
                volume_direction = price_change.apply(lambda x: 1 if x > 0 else (-1 if x < 0 else 0))
                obv = (df["æˆäº¤é‡"] * volume_direction).cumsum()
                result["series"]["OBV"] = {"x": df["æ—¥æœŸ"].tolist(), "y": [None if pd.isna(v) else float(v) for v in obv.tolist()]}

            if "ATR" in req.analyses and df is not None:
                logs.append("è®¡ç®—ATRå¹³å‡çœŸå®æ³¢å¹…â€¦")
                yield _sse_format("log", "è®¡ç®—ATRå¹³å‡çœŸå®æ³¢å¹…â€¦")
                # ATRè®¡ç®—ï¼šTR = max(H-L, |H-æ˜¨C|, |L-æ˜¨C|)
                high_low = df["æœ€é«˜"] - df["æœ€ä½"]
                high_close = (df["æœ€é«˜"] - df["æ”¶ç›˜"].shift(1)).abs()
                low_close = (df["æœ€ä½"] - df["æ”¶ç›˜"].shift(1)).abs()
                tr = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)
                atr = tr.rolling(window=14).mean()
                result["series"]["ATR"] = {"x": df["æ—¥æœŸ"].tolist(), "y": [None if pd.isna(v) else float(v) for v in atr.tolist()]}

            if "REALTIME" in req.analyses:
                yield _sse_format("log", "è·å–å®æ—¶è¡Œæƒ…â€¦")
                try:
                    spot = ak.stock_zh_a_spot_em()
                    row = spot[spot["ä»£ç "].astype(str) == processed_symbol]
                    if row.empty:
                        row = spot[spot["ä»£ç "].astype(str).str.contains(processed_symbol)]
                    if not row.empty:
                        # æ¸…ç†NaNå€¼
                        realtime_data = row.head(1).fillna(None).to_dict(orient="records")[0]
                        result["realtime"] = realtime_data
                    else:
                        result["realtime"] = None
                    yield _sse_format("log", f"å®æ—¶è¡Œæƒ…è·å–å®Œæˆ: {'1æ¡' if result['realtime'] else 'æ— æ•°æ®'}")
                except Exception:
                    result["realtime"] = None
                    yield _sse_format("log", "å®æ—¶è¡Œæƒ…è·å–å¤±è´¥ï¼Œå·²å¿½ç•¥")

            if "TOPUP" in req.analyses or "TOPDOWN" in req.analyses:
                yield _sse_format("log", "è·å–æ¶¨è·Œå¹…æ¦œâ€¦")
                try:
                    board = ak.stock_zh_a_spot_em()
                    board["æ¶¨è·Œå¹…"] = pd.to_numeric(board["æ¶¨è·Œå¹…"], errors="coerce")
                    board["æœ€æ–°ä»·"] = pd.to_numeric(board["æœ€æ–°ä»·"], errors="coerce")
                    if "TOPDOWN" in req.analyses:
                        board = board.sort_values("æ¶¨è·Œå¹…").head(10)
                    else:
                        board = board.sort_values("æ¶¨è·Œå¹…", ascending=False).head(10)
                    # æ¸…ç†NaNå€¼ï¼Œæ›¿æ¢ä¸ºNone
                    board_clean = board[["ä»£ç ", "åç§°", "æœ€æ–°ä»·", "æ¶¨è·Œå¹…"]].fillna(None)
                    result["board"] = board_clean.to_dict(orient="records")
                    yield _sse_format("log", f"æ¦œå•è·å–å®Œæˆ: {len(result['board']) if result['board'] else 0} æ¡")
                except Exception:
                    result["board"] = None
                    yield _sse_format("log", "æ¦œå•è·å–å¤±è´¥ï¼Œå·²å¿½ç•¥")

            if with_summary:
                yield _sse_format("log", "è°ƒç”¨LLMç”Ÿæˆç»¼åˆç»“è®ºâ€¦")
                try:
                    llm = ChatModelFactory.get_model(os.getenv("MODEL_NAME", "deepseek-chat"))
                    # æç‚¼äº‹å®ï¼ˆä¸éæµå¼ä¸€è‡´ï¼‰
                    facts_lines: list[str] = []
                    
                    # å¤šå‡çº¿åˆ†æ
                    if df is not None:
                        latest_close = float(df["æ”¶ç›˜"].tail(1).iloc[0])
                        ma_info = []
                        for ma_period in ["MA5", "MA20", "MA60"]:
                            if ma_period in req.analyses and ma_period in result["series"]:
                                latest_ma = result["series"][ma_period]["y"][-1]
                                if latest_ma is not None:
                                    rel = "ä¸Šæ–¹" if latest_close >= latest_ma else "ä¸‹æ–¹"
                                    ma_vals = [v for v in result["series"][ma_period]["y"] if v is not None]
                                    trend = None
                                    if len(ma_vals) >= 5:
                                        diff = ma_vals[-1] - ma_vals[-5]
                                        trend = "ä¸Šå‡" if diff > 0 else ("ä¸‹é™" if diff < 0 else "éœ‡è¡")
                                    ma_info.append(f"{ma_period}: {latest_ma:.2f} (ä»·æ ¼åœ¨{rel}, è¶‹åŠ¿{trend or 'æœªçŸ¥'})")
                        if ma_info:
                            facts_lines.append(f"å‡çº¿åˆ†æ: æ”¶ç›˜ {latest_close:.2f}, " + ", ".join(ma_info))
                    if "RSI14" in req.analyses and "RSI14" in result["series"]:
                        rsi_last = result["series"]["RSI14"]["y"][-1]
                        state = None
                        if rsi_last is not None:
                            state = "è¶…ä¹°" if rsi_last >= 70 else ("è¶…å–" if rsi_last <= 30 else "ä¸­æ€§")
                        facts_lines.append(f"RSI14: {rsi_last if rsi_last is not None else 'NA'} ({state or 'æœªçŸ¥'})")
                    if "MACD" in req.analyses and ("MACD" in result["series"] or "MACD_SIGNAL" in result["series"]):
                        macd_last = result["series"].get("MACD", {}).get("y", [None])[-1]
                        signal_last = result["series"].get("MACD_SIGNAL", {}).get("y", [None])[-1]
                        cross = None
                        if macd_last is not None and signal_last is not None:
                            cross = "é‡‘å‰" if macd_last >= signal_last else "æ­»å‰"
                        facts_lines.append(f"MACD: {macd_last if macd_last is not None else 'NA'}, Signal: {signal_last if signal_last is not None else 'NA'} ({cross or 'æœªçŸ¥'})")
                    if "BOLL" in req.analyses and all(k in result["series"] for k in ["BOLL_UP","BOLL_MID","BOLL_LOW"]) and df is not None:
                        up = result["series"]["BOLL_UP"]["y"][-1]
                        mid = result["series"]["BOLL_MID"]["y"][-1]
                        low = result["series"]["BOLL_LOW"]["y"][-1]
                        close_last = float(df["æ”¶ç›˜"].tail(1).iloc[0])
                        pos = None
                        if None not in [up, mid, low]:
                            if close_last >= up:
                                pos = "ä¸Šè½¨å¤–"
                            elif close_last >= mid:
                                pos = "ä¸Šè½¨-ä¸­è½¨"
                            elif close_last >= low:
                                pos = "ä¸­è½¨-ä¸‹è½¨"
                            else:
                                pos = "ä¸‹è½¨å¤–"
                        facts_lines.append(f"BOLL: ä¸Š {up if up is not None else 'NA'}, ä¸­ {mid if mid is not None else 'NA'}, ä¸‹ {low if low is not None else 'NA'}, ä»·æ ¼ä½ç½® {pos or 'æœªçŸ¥'}")
                    
                    # æ–°å¢æŒ‡æ ‡åˆ†æ
                    if "OBV" in req.analyses and "OBV" in result["series"]:
                        obv_last = result["series"]["OBV"]["y"][-1]
                        obv_values = [v for v in result["series"]["OBV"]["y"] if v is not None]
                        obv_trend = None
                        if len(obv_values) >= 10:
                            diff = obv_values[-1] - obv_values[-10]
                            obv_trend = "èµ„é‡‘æµå…¥" if diff > 0 else ("èµ„é‡‘æµå‡º" if diff < 0 else "éœ‡è¡")
                        facts_lines.append(f"OBVèƒ½é‡æ½®: {obv_last if obv_last is not None else 'NA'} (è¶‹åŠ¿: {obv_trend or 'æœªçŸ¥'})")
                    
                    if "ATR" in req.analyses and "ATR" in result["series"]:
                        atr_last = result["series"]["ATR"]["y"][-1]
                        if atr_last is not None and df is not None:
                            close_price = float(df["æ”¶ç›˜"].tail(1).iloc[0])
                            volatility = "é«˜" if atr_last > close_price * 0.02 else ("ä¸­" if atr_last > close_price * 0.01 else "ä½")
                            facts_lines.append(f"ATRæ³¢åŠ¨ç‡: {atr_last:.2f} (æ³¢åŠ¨æ€§: {volatility})")
                    
                    if "REALTIME" in req.analyses and result["realtime"]:
                        rt = result["realtime"]
                        facts_lines.append(f"å®æ—¶: åç§° {rt.get('åç§°','-')}, ä»·æ ¼ {rt.get('æœ€æ–°ä»·','-')}, æ¶¨è·Œå¹… {rt.get('æ¶¨è·Œå¹…','-')}%")

                    facts_text = "\n".join([f"- {line}" for line in facts_lines]) if facts_lines else "- æ— "
                    prompt = (
                        "ä½ æ˜¯è‚¡ç¥¨æŠ€æœ¯åˆ†æä¸“å®¶ã€‚è¯·åˆ†æä»¥ä¸‹æŒ‡æ ‡æ•°æ®ï¼Œä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¾“å‡ºï¼š\n\n"
                        "**è¾“å‡ºè¦æ±‚**ï¼š\n"
                        "1. å¿…é¡»æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼\n"
                        "2. ä¸è¦æœ‰ä»»ä½•è§£é‡Šæ–‡å­—ï¼Œåªè¾“å‡ºJSON\n"
                        "3. åŒ…å«ä»¥ä¸‹å­—æ®µï¼ˆå¿…é¡»å®Œæ•´ï¼‰ï¼š\n\n"
                        "```json\n"
                        "{\n"
                        '  "trend_signal": "å¤šå¤´",\n'
                        '  "confidence": 0.75,\n'
                        '  "key_signals": ["MA20ä¸Šå‡", "RSIä¸­æ€§"],\n'
                        '  "risk_points": ["ä»·æ ¼åœ¨MA20ä¸‹æ–¹", "æˆäº¤é‡ä¸è¶³"],\n'
                        '  "support_level": 10.8,\n'
                        '  "resistance_level": 11.5,\n'
                        '  "summary": "åŸºäºå½“å‰æŒ‡æ ‡ï¼Œè‚¡ä»·å¤„äºæŠ€æœ¯å›è°ƒé˜¶æ®µï¼Œä½†MA20å‘ä¸Šè¶‹åŠ¿ä¸ºä¸»å¯¼..."\n'
                        "}\n"
                        "```\n\n"
                        f"**åˆ†ææ ‡çš„**: {req.symbol}\n"
                        f"**æ—¶é—´åŒºé—´**: {start_date}è‡³{end_date}\n"
                        f"**æŠ€æœ¯æŒ‡æ ‡**: {', '.join(req.analyses)}\n\n"
                        "**æŒ‡æ ‡äº‹å®**:\n" + facts_text + "\n\n"
                        "è¯·æ ¹æ®ä¸Šè¿°æŒ‡æ ‡äº‹å®ï¼Œè¾“å‡ºJSONæ ¼å¼çš„åˆ†æç»“æœï¼š"
                    )
                    # æ˜¾ç¤ºLLMè¾“å…¥å†…å®¹
                    yield _sse_format("llm_input", prompt)
                    yield _sse_format("log", f"LLMæç¤ºé•¿åº¦: {len(prompt)}")

                    summary = llm.invoke(prompt)
                    content = getattr(summary, "content", str(summary))
                    
                    # å°è¯•è§£æç»“æ„åŒ–å“åº”ï¼Œå¦‚æœå¤±è´¥åˆ™ç”Ÿæˆé»˜è®¤ç»“æ„åŒ–æ•°æ®
                    try:
                        import json
                        import re
                        # æå–JSONéƒ¨åˆ†
                        json_match = re.search(r'\{.*\}', content, re.DOTALL)
                        if json_match:
                            structured_data = json.loads(json_match.group(0))
                            result["structured_analysis"] = structured_data
                            result["summary"] = structured_data.get("summary", content)
                            yield _sse_format("log", f"ç»“æ„åŒ–è§£ææˆåŠŸ: ç½®ä¿¡åº¦ {structured_data.get('confidence', 'N/A')}")
                        else:
                            # JSONè§£æå¤±è´¥ï¼ŒåŸºäºæŒ‡æ ‡ç”Ÿæˆé»˜è®¤ç»“æ„åŒ–æ•°æ®
                            yield _sse_format("log", "JSONè§£æå¤±è´¥ï¼Œç”Ÿæˆé»˜è®¤ç»“æ„åŒ–åˆ†æ")
                            structured_data = _generate_default_analysis(req.analyses, result, df)
                            result["structured_analysis"] = structured_data
                            result["summary"] = content
                            yield _sse_format("log", f"é»˜è®¤ç»“æ„åŒ–æ•°æ®ç”Ÿæˆå®Œæˆ: ç½®ä¿¡åº¦ {structured_data.get('confidence', 'N/A')}")
                    except Exception as parse_err:
                        # å¼‚å¸¸æƒ…å†µä¸‹ä¹Ÿç”Ÿæˆé»˜è®¤ç»“æ„åŒ–æ•°æ®
                        yield _sse_format("log", f"ç»“æ„åŒ–è§£æå¼‚å¸¸: {str(parse_err)}")
                        structured_data = _generate_default_analysis(req.analyses, result, df)
                        result["structured_analysis"] = structured_data
                        result["summary"] = content
                        yield _sse_format("log", f"é»˜è®¤ç»“æ„åŒ–æ•°æ®ç”Ÿæˆå®Œæˆ: ç½®ä¿¡åº¦ {structured_data.get('confidence', 'N/A')}")
                    
                    yield _sse_format("log", f"LLMè¾“å‡ºé¢„è§ˆ: {content[:120]}â€¦")
                except Exception as e:
                    result["summary"] = None
                    yield _sse_format("log", f"LLMç”Ÿæˆå¤±è´¥: {str(e)}")

            # ä»·æ ¼é¢„æµ‹
            if need_hist and df is not None:
                yield _sse_format("log", "ç”Ÿæˆä»·æ ¼é¢„æµ‹â€¦")
                try:
                    prediction = _predict_price_direction(df, result, req.analyses)
                    result["price_prediction"] = prediction
                    if "error" not in prediction:
                        yield _sse_format("log", f"é¢„æµ‹å®Œæˆ: ä¸Šæ¶¨æ¦‚ç‡ {prediction.get('up_probability', 'N/A')}%")
                    else:
                        yield _sse_format("log", f"é¢„æµ‹å¤±è´¥: {prediction['error']}")
                except Exception as e:
                    result["price_prediction"] = {"error": f"é¢„æµ‹å¼‚å¸¸: {str(e)}"}
                    yield _sse_format("log", f"ä»·æ ¼é¢„æµ‹å¼‚å¸¸: {str(e)}")

            # æœ€ç»ˆè¿”å›
            import json as _json
            from datetime import datetime, date

            # è‡ªå®šä¹‰JSONç¼–ç å™¨å¤„ç†æ—¥æœŸå¯¹è±¡
            class DateTimeEncoder(_json.JSONEncoder):
                def default(self, obj):
                    if isinstance(obj, (datetime, date)):
                        return obj.isoformat()
                    # å¤„ç†numpyæ•°å€¼ç±»å‹å’ŒNaNå€¼
                    import numpy as np
                    import math
                    # å¤„ç†Python float NaN/inf
                    if isinstance(obj, float):
                        if math.isnan(obj) or math.isinf(obj):
                            return None
                        return obj
                    # å¤„ç†numpyæ•°å€¼ç±»å‹å’ŒNaNå€¼
                    if hasattr(obj, 'dtype') and np.isscalar(obj):
                        if np.isnan(obj) or np.isinf(obj):
                            return None
                        return float(obj)
                    return super().default(obj)

            # æ¸…ç†NaNå€¼å¹¶åºåˆ—åŒ–
            cleaned_result = clean_nan_values(result)
            yield _sse_format("done", _json.dumps(cleaned_result, ensure_ascii=False, cls=DateTimeEncoder))
        except HTTPException as he:
            yield _sse_format("log", f"é”™è¯¯: {he.detail}")
            import json as _json
            yield _sse_format("done", _json.dumps({"error": he.detail}, ensure_ascii=False))
        except Exception as e:
            yield _sse_format("log", f"å¼‚å¸¸: {str(e)}")
            import json as _json
            yield _sse_format("done", _json.dumps({"error": "å›ºå®šåˆ†æå¤±è´¥"}, ensure_ascii=False))

    return StreamingResponse(gen(), media_type="text/event-stream")


# å…¼å®¹å¸¦å°¾æ–œæ çš„è·¯å¾„
@app.get("/api/fixed_analyse_stream/")
def fixed_analyse_stream_alias(symbol: str, analyses: str, lookback_days: int = 120, with_summary: bool = True):
    return fixed_analyse_stream(symbol=symbol, analyses=analyses, lookback_days=lookback_days, with_summary=with_summary)


# Qlibç›¸å…³APIç«¯ç‚¹
class QlibAnalysisRequest(BaseModel):
    symbol: str
    lookback_days: int = 120

@app.post("/api/qlib/analysis")
async def qlib_analysis(request: QlibAnalysisRequest):
    """Qlibé‡åŒ–åˆ†æ"""
    if not QLIB_AVAILABLE:
        raise HTTPException(status_code=503, detail="QlibåŠŸèƒ½ä¸å¯ç”¨")
    
    try:
        symbol = request.symbol.lower().replace("sh", "").replace("sz", "")
        start_date, end_date = _date_range(request.lookback_days)
        
        # è·å–å†å²æ•°æ®
        df = _fetch_hist(symbol, start_date, end_date)
        if df is None or df.empty:
            raise HTTPException(status_code=404, detail="æ— æ³•è·å–è‚¡ç¥¨æ•°æ®")
        
        # è¿è¡ŒQlibåˆ†æ
        result = demo_qlib_integration(request.symbol, df)
        
        return custom_json_response(result)
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Qlibåˆ†æå¤±è´¥: {str(e)}")

@app.get("/api/qlib/status")
async def qlib_status():
    """æ£€æŸ¥QlibçŠ¶æ€"""
    return {
        "qlib_available": _qlib.available,
        "enabled": _qlib.enabled,
        "provider_uri": _qlib.provider_uri,
        "region": _qlib.region,
        "error": _qlib.error,
        "version": "m1-demo",
        "features": [
            "ç‰¹å¾å·¥ç¨‹",
            "é‡åŒ–é¢„æµ‹",
            "ç®€å•å›æµ‹",
            "Alphaå› å­è®¡ç®—",
            "FeatureStoreç¼“å­˜",
        ] if _qlib.available else []
    }


class QlibFeaturesRequest(BaseModel):
    symbol: str
    analyses: List[str]
    lookback_days: int = 120
    freq: str = "day"


@app.post("/api/qlib/features", summary="Qlib Features")
async def qlib_features(req: QlibFeaturesRequest):
    try:
        symbol = req.symbol.lower().replace("sh", "").replace("sz", "")
        start_date, end_date = _date_range(req.lookback_days or 120)
        factor_set = ",".join(sorted(req.analyses))

        # å…ˆæŸ¥ç¼“å­˜
        cached = feature_store.get(symbol, start_date, end_date, req.freq, factor_set)
        if cached is not None:
            return {"series": cached, "cached": True}

        # è·å–å†å²æ•°æ®
        df = _fetch_hist(symbol, start_date, end_date)
        if df is None or df.empty:
            raise HTTPException(status_code=404, detail="æ— æ³•è·å–è‚¡ç¥¨æ•°æ®")

        # è®¡ç®—å› å­ï¼ˆå½“å‰ç”¨Pandaså®ç°ï¼›æœªæ¥å¯åˆ‡æ¢Qlibè¡¨è¾¾å¼ï¼‰
        series = build_factors(df, req.analyses)

        # å†™å…¥ç¼“å­˜
        feature_store.set(symbol, start_date, end_date, req.freq, factor_set, series, ttl_seconds=1800)

        return {"series": series, "cached": False}
    except HTTPException:
        raise
    except Exception as e:
        # è¿”å›è¯¦ç»†é”™è¯¯ï¼Œä¾¿äºå®šä½
        raise HTTPException(status_code=500, detail=f"qlib_featureså¤±è´¥: {str(e)}")


@app.get("/api/qlib/features_stream")
def qlib_features_stream(symbol: str, analyses: str, lookback_days: int = 120, freq: str = "day"):
    import time
    import json as _json

    def gen():
        t0 = time.time()
        try:
            yield _sse_format("status", "å‡†å¤‡æ„å»ºå› å­â€¦")
            symbol_clean = symbol.lower().replace("sh", "").replace("sz", "")
            start_date, end_date = _date_range(lookback_days or 120)
            factor_set = ",".join(sorted(analyses.split(",")))
            yield _sse_format("meta", _json.dumps({
                "symbol": symbol, "start": start_date, "end": end_date, "analyses": factor_set
            }, ensure_ascii=False))

            # ç¼“å­˜å‘½ä¸­
            cached = feature_store.get(symbol_clean, start_date, end_date, freq, factor_set)
            if cached is not None:
                dt = (time.time() - t0) * 1000
                yield _sse_format("cache_hit", _json.dumps({"cached": True, "elapsed_ms": round(dt, 1)}))
                yield _sse_format("done", _json.dumps({"series": cached, "cached": True}, ensure_ascii=False))
                return

            # è·å–å†å²æ•°æ®
            yield _sse_format("status", "æ‹‰å–å†å²æ•°æ®â€¦")
            df = _fetch_hist(symbol_clean, start_date, end_date)
            if df is None or df.empty:
                yield _sse_format("error", "æ— æ³•è·å–è‚¡ç¥¨æ•°æ®")
                yield _sse_format("done", _json.dumps({"error": "æ— æ³•è·å–è‚¡ç¥¨æ•°æ®"}, ensure_ascii=False))
                return

            # è®¡ç®—å› å­
            yield _sse_format("status", "è®¡ç®—å› å­â€¦")
            t1 = time.time()
            series = build_factors(df, analyses.split(","))
            dt_build = (time.time() - t1) * 1000
            yield _sse_format("built", _json.dumps({
                "factors": list(series.keys()),
                "elapsed_ms": round(dt_build, 1)
            }, ensure_ascii=False))

            # å†™å…¥ç¼“å­˜
            feature_store.set(symbol_clean, start_date, end_date, freq, factor_set, series, ttl_seconds=1800)
            total_ms = (time.time() - t0) * 1000
            yield _sse_format("done", _json.dumps({
                "series": series, "cached": False, "elapsed_ms": round(total_ms, 1)
            }, ensure_ascii=False))
        except Exception as e:
            yield _sse_format("error", str(e))
            yield _sse_format("done", _json.dumps({"error": str(e)}, ensure_ascii=False))

    return StreamingResponse(gen(), media_type="text/event-stream")


class QlibAnalyseRequest(BaseModel):
    symbol: str
    analyses: List[str]
    lookback_days: int = 120
    horizon_days: int = 5


@app.post("/api/qlib/analyse")
async def qlib_analyse(req: QlibAnalyseRequest):
    """è®¡ç®—å› å­+ä»·æ ¼é¢„æµ‹+ç»“æ„åŒ–ç»“è®º+åŸºç¡€é£é™©ï¼ˆä¸å‰ç«¯ç»“æ„å¯¹é½ï¼‰ã€‚"""
    try:
        symbol_clean = req.symbol.lower().replace("sh", "").replace("sz", "")
        start_date, end_date = _date_range(req.lookback_days or 120)

        # å†å²æ•°æ®
        df = _fetch_hist(symbol_clean, start_date, end_date)
        if df is None or df.empty:
            raise HTTPException(status_code=404, detail="æ— æ³•è·å–è‚¡ç¥¨æ•°æ®")

        # å› å­/æŒ‡æ ‡åºåˆ—
        series = build_factors(df, req.analyses)

        # ä»·æ ¼é¢„æµ‹ï¼ˆå¤ç”¨ç°æœ‰ä¼˜åŒ–ç®—æ³•ï¼‰
        pred = _predict_price_direction(df, {"series": series}, req.analyses)

        # ç»“æ„åŒ–ç»“è®ºï¼ˆä¼˜å…ˆè°ƒç”¨å·²æœ‰çš„é»˜è®¤ç”Ÿæˆé€»è¾‘ï¼‰
        try:
            structured = _generate_default_analysis(req.analyses, {"series": series, "indicators": {}}, df)
        except Exception:
            # å›é€€ï¼šæ ¹æ®ç®€å•è§„åˆ™ç”Ÿæˆ
            structured = {
                "trend_signal": "éœ‡è¡",
                "confidence": 0.5,
                "key_signals": [],
                "risk_points": []
            }
            try:
                close_last = float(df["æ”¶ç›˜"].tail(1).iloc[0])
                ma20_last = series.get("MA20", {}).get("y", [None])[-1]
                if ma20_last is not None:
                    structured["trend_signal"] = "å¤šå¤´" if close_last >= ma20_last else "ç©ºå¤´"
            except Exception:
                pass

        # åŸºç¡€é£é™©æŒ‡æ ‡
        import numpy as _np
        ret = df["æ”¶ç›˜"].pct_change().dropna().values
        if ret.size > 0:
            mdd = 0.0
            equity = (1 + ret).cumprod()
            peak = _np.maximum.accumulate(equity)
            mdd = float(_np.max((peak - equity) / peak)) if equity.size > 0 else 0.0
            vol = float(_np.std(ret) * (252 ** 0.5))
            var_95 = float(_np.percentile(ret, 5))
            win_rate = float((_np.sum(ret > 0) / ret.size))
        else:
            mdd = vol = var_95 = win_rate = 0.0

        risk = {
            "max_drawdown": mdd,
            "volatility": vol,
            "var_95": var_95,
            "win_rate": win_rate,
        }

        return custom_json_response({
            "meta": {"symbol": req.symbol, "start": start_date, "end": end_date, "analyses": req.analyses},
            "series": series,
            "structured_analysis": structured,
            "price_prediction": pred,
            "risk": risk,
        })
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"qlib_analyseå¤±è´¥: {str(e)}")




class GenerateDataRequest(BaseModel):
    symbols: str = "600519"
    days: int = 100
    start_date: Optional[str] = None
    end_date: Optional[str] = None

# æŒä»“ç®¡ç†ç›¸å…³æ•°æ®æ¨¡å‹
class TransactionRequest(BaseModel):
    symbol: str
    name: str
    action: str  # 'buy' or 'sell'
    quantity: int
    price: float
    fee: float = 0
    note: str = ""

class PositionEditRequest(BaseModel):
    symbol: str
    quantity: int
    avg_cost: float
    note: str = ""

class BatchTransactionRequest(BaseModel):
    transactions: List[TransactionRequest]

class PositionResponse(BaseModel):
    symbol: str
    name: str
    quantity: int
    avg_cost: float
    current_price: float
    market_value: float
    profit_loss: float
    profit_loss_pct: float
    last_updated: str

class PortfolioResponse(BaseModel):
    total_assets: float
    available_cash: float
    market_value: float
    total_profit_loss: float
    total_profit_loss_pct: float
    positions: List[PositionResponse]
    position_count: int



# ==================== ç­–ç•¥å›æµ‹API ====================

@app.post("/api/strategy/run_backtest")
def run_strategy_backtest(
    buy_threshold: float = 0.6,
    sell_threshold: float = 0.4,
    max_positions: int = 10,
    initial_capital: float = 100000.0,
    transaction_cost: float = 0.002,
    position_size: float = 0.1,
    start_date: str = None,
    end_date: str = None,
    symbols: str = None,  # é€—å·åˆ†éš”çš„è‚¡ç¥¨ä»£ç 
    selection_mode: str = "threshold",  # ğŸ”¥ æ–°å¢ï¼šäº¤æ˜“é€‰æ‹©æ¨¡å¼ï¼ˆthreshold/topkï¼‰
    top_k: int = 3,  # ğŸ”¥ æ–°å¢ï¼štop-kæ¨¡å¼ä¸‹çš„æ¯æ—¥é€‰æ‹©æ•°
    strategy_mode: str = "standard",  # ğŸš€ æ–°å¢ï¼šç­–ç•¥æ¨¡å¼ï¼ˆstandard/high_returnï¼‰
    days: int = None,  # ğŸš€ é«˜æ”¶ç›Šæ¨¡å¼çš„å›æµ‹å¤©æ•°
    force_refresh: bool = False  # ğŸ”„ æ–°å¢ï¼šå¼ºåˆ¶åˆ·æ–°å†å²æ•°æ®
):
    """è¿è¡Œç­–ç•¥å›æµ‹"""
    try:
        # ğŸš€ æ£€æŸ¥æ˜¯å¦ä½¿ç”¨é«˜æ”¶ç›Šæ¨¡å¼
        if strategy_mode == "high_return":
            # ä½¿ç”¨å¢å¼ºç­–ç•¥ï¼ˆåŸºäºæ–‡æ¡£å®Œæ•´ç®—æ³•ï¼‰
            from backend.enhanced_strategy import EnhancedStrategy, EnhancedStrategyConfig
            from backend.opening_prediction import OpeningPredictor, format_prediction_result
            
            # è§£æè‚¡ç¥¨ä»£ç 
            symbol_list = None
            if symbols:
                symbol_list = [s.strip() for s in symbols.split(',') if s.strip()]
            
            # ä½¿ç”¨è°ƒæ•´åçš„å‚æ•°é…ç½®ï¼ˆé€‚åˆçŸ­æœŸå›æµ‹ï¼‰
            enhanced_config = EnhancedStrategyConfig(
                buy_threshold=0.55,      # é™ä½ä¹°å…¥é˜ˆå€¼ä»¥å¢åŠ äº¤æ˜“æœºä¼š
                sell_threshold=0.45,     # æé«˜å–å‡ºé˜ˆå€¼ä»¥å‡å°‘è¿‡æ—©å–å‡º
                position_size=0.12,      # ä¿æŒæ–‡æ¡£æœ€ä¼˜å€¼
                max_positions=10,
                initial_capital=initial_capital,
                transaction_cost=transaction_cost
            )
            
            # åˆå§‹åŒ–å¢å¼ºç­–ç•¥
            strategy = EnhancedStrategy(enhanced_config)
            
            try:
                # åŠ è½½é¢„æµ‹æ•°æ®å’Œä»·æ ¼æ•°æ®
                predictions = load_predictions_for_enhanced_strategy(symbol_list, start_date, end_date, days)
                price_data = load_price_data_for_enhanced_strategy(symbol_list, start_date, end_date, days)
                
                if predictions.empty or price_data.empty:
                    logger.warning("å¢å¼ºç­–ç•¥æ•°æ®ä¸è¶³ï¼Œé™çº§åˆ°åŸºç¡€é«˜æ”¶ç›Šç­–ç•¥")
                    # é™çº§åˆ°åŸé«˜æ”¶ç›Šç­–ç•¥
                    from backend.high_return_strategy import HighReturnStrategy
                    fallback_strategy = HighReturnStrategy()
                    results = fallback_strategy.run_backtest(symbols=symbol_list, days=days or 365, force_refresh=force_refresh)
                    
                    return custom_json_response({
                        "success": True,
                        "strategy_name": "ğŸš€ é«˜æ”¶ç›Šæ¨¡å¼ï¼ˆé™çº§ç‰ˆï¼‰",
                        "results": results,
                        "performance_rating": _get_performance_rating(results),
                        "timestamp": _json.dumps(datetime.now(), default=str),
                        "warning": "å¢å¼ºç­–ç•¥æ•°æ®ä¸è¶³ï¼Œå·²è‡ªåŠ¨é™çº§åˆ°åŸºç¡€é«˜æ”¶ç›Šç­–ç•¥"
                    })
                
                # è¿è¡Œå¢å¼ºç­–ç•¥å›æµ‹
                results = strategy.run_backtest(predictions, price_data, start_date, end_date)
                
                return custom_json_response({
                    "success": True,
                    "strategy_name": "ğŸ“ˆ å¢å¼ºç­–ç•¥ï¼ˆæ–‡æ¡£å®Œæ•´ç‰ˆï¼‰",
                    "results": results,
                    "performance_rating": _get_performance_rating(results.get('performance_metrics', {})),
                    "timestamp": _json.dumps(datetime.now(), default=str)
                })
                
            except Exception as e:
                logger.error(f"å¢å¼ºç­–ç•¥æ‰§è¡Œå¤±è´¥: {e}")
                # é™çº§åˆ°åŸé«˜æ”¶ç›Šç­–ç•¥
                from backend.high_return_strategy import HighReturnStrategy
                fallback_strategy = HighReturnStrategy()
                results = fallback_strategy.run_backtest(symbols=symbol_list, days=days or 365, force_refresh=force_refresh)
                
                return custom_json_response({
                    "success": True,
                    "strategy_name": "ğŸš€ é«˜æ”¶ç›Šæ¨¡å¼ï¼ˆé™çº§ç‰ˆï¼‰",
                    "results": results,
                    "performance_rating": _get_performance_rating(results),
                    "timestamp": _json.dumps(datetime.now(), default=str),
                    "warning": "å¢å¼ºç­–ç•¥æ•°æ®ä¸è¶³ï¼Œå·²é™çº§åˆ°åŸºç¡€é«˜æ”¶ç›Šç­–ç•¥"
                })
        
        # æ ‡å‡†æ¨¡å¼ï¼šä½¿ç”¨åŸæœ‰é€»è¾‘
        config = StrategyConfig(
            buy_threshold=buy_threshold,
            sell_threshold=sell_threshold,
            max_positions=max_positions,
            initial_capital=initial_capital,
            transaction_cost=transaction_cost,
            position_size=position_size,
            selection_mode=selection_mode,
            top_k=top_k
        )
        
        # è§£æè‚¡ç¥¨ä»£ç 
        symbol_list = None
        if symbols:
            symbol_list = [s.strip() for s in symbols.split(',') if s.strip()]
        
        # é¦–æ¬¡å°è¯•è¿è¡Œå›æµ‹
        result = backtester.run_backtest(config, start_date, end_date, symbol_list, force_refresh=force_refresh)
        
        # å¦‚æœæ²¡æœ‰å†å²æ•°æ®ï¼Œè‡ªåŠ¨ç”Ÿæˆç„¶åé‡æ–°è¿è¡Œå›æµ‹
        if isinstance(result, dict) and result.get("error") == "æ²¡æœ‰å¯ç”¨çš„é¢„æµ‹æ•°æ®":
            coverage_details = result.get("details", {})
            coverage_reason = coverage_details.get("reason", "æ•°æ®ä¸è¶³")
            print(f"âš ï¸ {coverage_reason}ï¼Œå¼€å§‹è‡ªåŠ¨ç”Ÿæˆ...")
            
            # ç¡®å®šè¦ç”Ÿæˆæ•°æ®çš„è‚¡ç¥¨å’Œæ—¶é—´èŒƒå›´
            target_symbols = symbols if symbols else "600519,000001,000002"  # é»˜è®¤è‚¡ç¥¨
            
            try:
                # å¦‚æœæŒ‡å®šäº†æ—¥æœŸèŒƒå›´ï¼Œä½¿ç”¨æ—¥æœŸèŒƒå›´ç”Ÿæˆ
                if start_date and end_date:
                    print(f"ğŸ”„ è‡ªåŠ¨ç”Ÿæˆå†å²æ•°æ®: {target_symbols} ({start_date} ~ {end_date})")
                    generate_result = calibrator.generate_historical_backtest_data_by_date_range(
                        target_symbols, start_date, end_date
                    )
                else:
                    # å¦åˆ™ä½¿ç”¨é»˜è®¤å¤©æ•°ç”Ÿæˆ
                    days = 365  # é»˜è®¤1å¹´ï¼Œæ”¯æŒæ›´é•¿æœŸå›æµ‹
                    print(f"ğŸ”„ è‡ªåŠ¨ç”Ÿæˆå†å²æ•°æ®: {target_symbols} ({days}å¤©)")
                    generate_result = calibrator.generate_historical_backtest_data(target_symbols, days)
                
                if generate_result and isinstance(generate_result, dict):
                    success_count = len(generate_result.get("success_symbols", []))
                    if success_count > 0:
                        print(f"âœ… è‡ªåŠ¨ç”ŸæˆæˆåŠŸï¼Œè·å¾— {generate_result.get('total_records', 0)} æ¡å†å²æ•°æ®")
                        
                        # é‡æ–°è¿è¡Œå›æµ‹
                        print("ğŸ”„ é‡æ–°è¿è¡Œå›æµ‹...")
                        result = backtester.run_backtest(config, start_date, end_date, symbol_list, force_refresh=force_refresh)
                        
                        # åœ¨ç»“æœä¸­æ·»åŠ è‡ªåŠ¨ç”Ÿæˆçš„æ ‡è®°
                        if isinstance(result, dict) and "error" not in result:
                            result["auto_generated_data"] = True
                            result["generated_records"] = generate_result.get('total_records', 0)
                            result["generated_symbols"] = generate_result.get('success_symbols', [])
                            result["message"] = f"è‡ªåŠ¨ç”Ÿæˆäº† {generate_result.get('total_records', 0)} æ¡å†å²æ•°æ®å¹¶å®Œæˆå›æµ‹"
                    else:
                        return custom_json_response({
                            "error": "è‡ªåŠ¨ç”Ÿæˆå†å²æ•°æ®å¤±è´¥ï¼Œæ— æ³•è·å–è‚¡ç¥¨æ•°æ®ï¼Œè¯·æ£€æŸ¥è‚¡ç¥¨ä»£ç æ˜¯å¦æ­£ç¡®"
                        })
                else:
                    return custom_json_response({
                        "error": "è‡ªåŠ¨ç”Ÿæˆå†å²æ•°æ®å¤±è´¥ï¼Œè¯·æ‰‹åŠ¨ç”Ÿæˆå†å²æ•°æ®åé‡è¯•"
                    })
                    
            except Exception as gen_error:
                print(f"âŒ è‡ªåŠ¨ç”Ÿæˆå†å²æ•°æ®å¼‚å¸¸: {gen_error}")
                return custom_json_response({
                    "error": f"è‡ªåŠ¨ç”Ÿæˆå†å²æ•°æ®å¤±è´¥: {str(gen_error)}ï¼Œè¯·æ‰‹åŠ¨ç”Ÿæˆå†å²æ•°æ®åé‡è¯•"
                })
        
        # ä¸ºæ ‡å‡†æ¨¡å¼æ·»åŠ successå­—æ®µä»¥ä¿æŒAPIä¸€è‡´æ€§
        if isinstance(result, dict) and "error" not in result:
            result["success"] = True
        return custom_json_response(result)
        
    except Exception as e:
        return custom_json_response({"error": f"ç­–ç•¥å›æµ‹å¤±è´¥: {str(e)}"})

@app.get("/api/strategy/performance/{start_date}/{end_date}")
def get_strategy_performance(start_date: str, end_date: str):
    """è·å–æŒ‡å®šæ—¶é—´æ®µçš„ç­–ç•¥è¡¨ç°"""
    try:
        config = StrategyConfig()  # ä½¿ç”¨é»˜è®¤é…ç½®
        result = backtester.run_backtest(config, start_date, end_date, force_refresh=False)
        
        if "error" in result:
            return custom_json_response(result)
            
        # åªè¿”å›æ€§èƒ½æŒ‡æ ‡
        return custom_json_response({
            "performance_metrics": result["performance_metrics"],
            "total_trades": result["total_trades"],
            "final_portfolio_value": result["final_portfolio_value"]
        })
        
    except Exception as e:
        return custom_json_response({"error": f"è·å–ç­–ç•¥è¡¨ç°å¤±è´¥: {str(e)}"})

@app.post("/api/strategy/optimize")
def optimize_strategy(
    start_date: str = None,
    end_date: str = None,
    symbols: str = None,
):
    """ç­–ç•¥å‚æ•°ä¼˜åŒ–ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
    from datetime import datetime
    print(f"ğŸ”§ ç­–ç•¥ä¼˜åŒ–APIè¢«è°ƒç”¨ - æ—¶é—´: {datetime.now()}")
    
    # æ·»åŠ ç®€å•çš„é˜²é‡å¤è°ƒç”¨æœºåˆ¶
    import time
    if not hasattr(optimize_strategy, 'last_call_time'):
        optimize_strategy.last_call_time = 0
    
    current_time = time.time()
    if current_time - optimize_strategy.last_call_time < 5:  # 5ç§’å†…ä¸å…è®¸é‡å¤è°ƒç”¨
        print(f"âš ï¸ ä¼˜åŒ–APIè°ƒç”¨è¿‡äºé¢‘ç¹ï¼Œå¿½ç•¥é‡å¤è¯·æ±‚")
        return custom_json_response({"error": "è¯·å‹¿é¢‘ç¹è°ƒç”¨ä¼˜åŒ–åŠŸèƒ½ï¼Œè¯·ç­‰å¾…5ç§’åå†è¯•"})
    
    optimize_strategy.last_call_time = current_time
    try:
        best_config = None
        best_return = -float('inf')
        best_sharpe = -float('inf')
        optimization_results = []
        
        # è§£æè‚¡ç¥¨ä»£ç 
        symbol_list = None
        if symbols:
            symbol_list = [s.strip() for s in symbols.split(',') if s.strip()]
        
        print(f"ğŸ¯ ä¼˜åŒ–å‚æ•°: start_date={start_date}, end_date={end_date}, symbols={symbols}")

        # è‹¥æä¾›èŒƒå›´ä¸è‚¡ç¥¨ï¼Œé¢„ç”Ÿæˆå†å²é¢„æµ‹æ•°æ®ï¼ˆAKShareè¡¥å……ï¼‰
        try:
            if start_date and end_date and symbols:
                print(f"ğŸ§° é¢„ç”Ÿæˆå†å²é¢„æµ‹æ•°æ®: {symbols} ({start_date} ~ {end_date})")
                _pre = calibrator.generate_historical_backtest_data_by_date_range(symbols, start_date, end_date)
                print(f"ğŸ§° é¢„ç”Ÿæˆå®Œæˆ: {_pre}")
        except Exception as _e:
            print(f"âš ï¸ é¢„ç”Ÿæˆå¤±è´¥ï¼Œç»§ç»­ä¼˜åŒ–: {_e}")

        # å‚æ•°ç©ºé—´ï¼ˆå…¨å‚æ•°ï¼‰ï¼š
        buy_thresholds = [0.5, 0.55, 0.6, 0.65]
        sell_thresholds = [0.25, 0.3, 0.35, 0.4]
        gamma_list = [0.8, 1.0, 1.2, 1.5, 2.0]
        selection_modes = ["threshold", "topk", "aggressive"]
        topk_list = [2, 3, 5]
        # æ¿€è¿›æ¨¡å¼å‚æ•°
        momentum_weights = [0.2, 0.3, 0.5]
        profit_targets = [0.03, 0.05, 0.08]

        for mode in selection_modes:
            for buy_th in buy_thresholds:
                for sell_th in sell_thresholds:
                    if mode == "threshold" and buy_th <= sell_th:
                        continue
                    for gamma in gamma_list:
                        if mode == "aggressive":
                            for mw in momentum_weights:
                                for pt in profit_targets:
                                    config = StrategyConfig(
                                        buy_threshold=buy_th,
                                        sell_threshold=sell_th,
                                        selection_mode=mode,
                                        momentum_weight=mw,
                                        profit_target=pt
                                    )
                                    backtester.verbose = False
                                    result = backtester.run_backtest(config, start_date, end_date, symbol_list)
                                    if "error" in result:
                                        continue
                                    total_return = result["performance_metrics"]["total_return"]
                                    sharpe_ratio = result["performance_metrics"]["sharpe_ratio"]
                                    score = total_return  # ä»¥æœ€é«˜æ”¶ç›Šä¸ºç›®æ ‡
                                    optimization_results.append({
                                        "selection_mode": mode,
                                        "buy_threshold": buy_th,
                                        "sell_threshold": sell_th,
                                        "momentum_weight": mw,
                                        "profit_target": pt,
                                        "total_return": total_return,
                                        "sharpe_ratio": sharpe_ratio,
                                        "score": score
                                    })
                                    if (score > best_return) or (score == best_return and sharpe_ratio > best_sharpe):
                                        best_return = score
                                        best_sharpe = sharpe_ratio
                                        best_config = config
                        elif mode == "topk":
                            for tk in topk_list:
                                config = StrategyConfig(
                                    buy_threshold=buy_th,
                                    sell_threshold=sell_th,
                                    selection_mode=mode,
                                    top_k=tk
                                )
                                # é™é»˜æ¨¡å¼è¿è¡Œï¼ˆå‡å°‘æ—¥å¿—åˆ·å±ï¼‰
                                backtester.verbose = False
                                result = backtester.run_backtest(config, start_date, end_date, symbol_list)
                                if "error" in result:
                                    continue
                                total_return = result["performance_metrics"]["total_return"]
                                sharpe_ratio = result["performance_metrics"]["sharpe_ratio"]
                                score = total_return  # ä»¥æœ€é«˜æ”¶ç›Šä¸ºç›®æ ‡
                                optimization_results.append({
                                    "selection_mode": mode,
                                    "top_k": tk,
                                    "buy_threshold": buy_th,
                                    "sell_threshold": sell_th,
                                    "total_return": total_return,
                                    "sharpe_ratio": sharpe_ratio,
                                    "score": score
                                })
                                if (score > best_return) or (score == best_return and sharpe_ratio > best_sharpe):
                                    best_return = score
                                    best_sharpe = sharpe_ratio
                                    best_config = config
                        else:
                            config = StrategyConfig(
                                buy_threshold=buy_th,
                                sell_threshold=sell_th,
                                selection_mode=mode
                            )
                            backtester.verbose = False
                            result = backtester.run_backtest(config, start_date, end_date, symbol_list)
                            if "error" in result:
                                continue
                            total_return = result["performance_metrics"]["total_return"]
                            sharpe_ratio = result["performance_metrics"]["sharpe_ratio"]
                            score = total_return
                            optimization_results.append({
                                "selection_mode": mode,
                                "buy_threshold": buy_th,
                                "sell_threshold": sell_th,
                                "total_return": total_return,
                                "sharpe_ratio": sharpe_ratio,
                                "score": score
                            })
                            if (score > best_return) or (score == best_return and sharpe_ratio > best_sharpe):
                                best_return = score
                                best_sharpe = sharpe_ratio
                                best_config = config
        
        # ä½¿ç”¨æœ€ä¼˜å‚æ•°è¿è¡Œå®Œæ•´å›æµ‹
        if best_config:
            best_result = backtester.run_backtest(best_config, start_date, end_date, symbol_list)
            return custom_json_response({
                "best_config": best_config.__dict__,
                "best_performance": best_result["performance_metrics"],
                "optimization_results": optimization_results
            })
        else:
            return custom_json_response({"error": "ä¼˜åŒ–å¤±è´¥ï¼Œæ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„å‚æ•°ç»„åˆ"})
            
    except Exception as e:
        return custom_json_response({"error": f"ç­–ç•¥ä¼˜åŒ–å¤±è´¥: {str(e)}"})

# ==================== è‚¡ç¥¨æ± ç®¡ç†API ====================

@app.get("/api/strategy/available_stocks")
def get_available_stocks():
    """è·å–å¯ç”¨çš„è‚¡ç¥¨åˆ—è¡¨"""
    try:
        conn = sqlite3.connect("calibration.db")
        cursor = conn.cursor()
        
        cursor.execute("""
            SELECT DISTINCT symbol, COUNT(*) as prediction_count,
                   MIN(prediction_date) as first_date,
                   MAX(prediction_date) as last_date
            FROM predictions 
            WHERE actual_direction IS NOT NULL
            GROUP BY symbol
            ORDER BY prediction_count DESC
        """)
        
        stocks = []
        for row in cursor.fetchall():
            symbol, count, first_date, last_date = row
            stocks.append({
                "symbol": symbol,
                "prediction_count": count,
                "first_date": first_date,
                "last_date": last_date,
                "name": get_stock_name(symbol)  # è·å–è‚¡ç¥¨åç§°
            })
        
        conn.close()
        return custom_json_response({"available_stocks": stocks})
        
    except Exception as e:
        return custom_json_response({"error": f"è·å–è‚¡ç¥¨åˆ—è¡¨å¤±è´¥: {str(e)}"})

def get_stock_name(symbol: str) -> str:
    """è·å–è‚¡ç¥¨åç§°ï¼ˆç¼“å­˜ä¼˜å…ˆ â†’ Akshare å•è‚¡ä¿¡æ¯ï¼‰"""
    try:
        # ä¼˜å…ˆä½¿ç”¨ PortfolioManager çš„å…¨å±€ç¼“å­˜
        try:
            from backend.portfolio_manager import stock_name_cache  # type: ignore
        except Exception:
            stock_name_cache = None  # noqa: F841

        if 'stock_name_cache' in locals() and stock_name_cache:
            cached = stock_name_cache.get(symbol)
            if cached:
                return cached

        # ç›´æ¥è°ƒç”¨ Akshare å•åªè‚¡ç¥¨ä¿¡æ¯æ¥å£
        if ak is None:
            return symbol
        info = ak.stock_individual_info_em(symbol=symbol)
        if info is not None and not info.empty:
            row = info[info['item'] == 'è‚¡ç¥¨ç®€ç§°']
            if not row.empty:
                name = row['value'].iloc[0]
                if name and name != symbol:
                    if 'stock_name_cache' in locals() and stock_name_cache:
                        stock_name_cache.set(symbol, name)
                    return name
        return symbol
    except Exception:
        return symbol

# å…¬å¼€æ¥å£ï¼šè·å–å•ä¸ªè‚¡ç¥¨åç§°
@app.get("/api/stock_name/{symbol}")
def api_get_stock_name(symbol: str):
    try:
        return custom_json_response({"symbol": symbol, "name": get_stock_name(symbol)})
    except Exception as e:
        return custom_json_response({"symbol": symbol, "name": symbol, "error": str(e)})

# è‚¡ç¥¨æ± é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆä¿ç•™ç”¨äºå…¼å®¹æ€§ï¼‰
STOCK_POOLS_CONFIG_FILE = "stock_pools_config.json"

# å¯¼å…¥æ•°æ®åº“ç®¡ç†æ¨¡å—
USE_DATABASE = False
try:
    import sys
    import os
    # æ·»åŠ backendç›®å½•åˆ°Pythonè·¯å¾„
    backend_dir = os.path.dirname(os.path.abspath(__file__))
    if backend_dir not in sys.path:
        sys.path.insert(0, backend_dir)
    
    from stock_pool_db import get_stock_pool_db
    USE_DATABASE = True
    logger.info("âœ… è‚¡ç¥¨æ± æ•°æ®åº“æ¨¡å—åŠ è½½æˆåŠŸ")
except ImportError as e:
    logger.warning(f"âš ï¸ è‚¡ç¥¨æ± æ•°æ®åº“æ¨¡å—åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨JSONæ–‡ä»¶: {e}")
    USE_DATABASE = False
except Exception as e:
    logger.error(f"âŒ è‚¡ç¥¨æ± æ•°æ®åº“æ¨¡å—åŠ è½½å¼‚å¸¸: {e}")
    USE_DATABASE = False

def load_stock_pools():
    """åŠ è½½è‚¡ç¥¨æ± é…ç½®ï¼ˆæ•°æ®åº“ä¼˜å…ˆï¼‰"""
    
    # ä¼˜å…ˆä½¿ç”¨æ•°æ®åº“
    if USE_DATABASE:
        try:
            db = get_stock_pool_db()
            data = db.get_all_pools()
            if data:  # å¦‚æœæ•°æ®åº“æœ‰æ•°æ®
                logger.info(f"âœ… ä»æ•°æ®åº“åŠ è½½è‚¡ç¥¨æ± é…ç½®ï¼ŒåŒ…å«{len(data)}ä¸ªè‚¡ç¥¨æ± : {list(data.keys())}")
                return data
            else:
                logger.info("ğŸ“Š æ•°æ®åº“ä¸ºç©ºï¼Œå°è¯•ä»JSONæ–‡ä»¶è¿ç§»æ•°æ®")
                # å¦‚æœæ•°æ®åº“ä¸ºç©ºï¼Œå°è¯•ä»JSONè¿ç§»ä¸€æ¬¡æ€§æ•°æ®
                if os.path.exists(STOCK_POOLS_CONFIG_FILE):
                    try:
                        if db.migrate_from_json(STOCK_POOLS_CONFIG_FILE):
                            logger.info("âœ… JSONæ•°æ®è¿ç§»æˆåŠŸï¼Œé‡æ–°ä»æ•°æ®åº“åŠ è½½")
                            data = db.get_all_pools()
                            if data:
                                return data
                        else:
                            logger.error("âŒ JSONæ•°æ®è¿ç§»å¤±è´¥")
                    except Exception as e:
                        logger.error(f"JSONæ•°æ®è¿ç§»å¼‚å¸¸: {e}")
        except Exception as e:
            logger.error(f"âŒ æ•°æ®åº“æ“ä½œå¤±è´¥: {e}")
    
    # å¦‚æœæ•°æ®åº“ä¸å¯ç”¨ï¼Œæ‰ä½¿ç”¨JSONæ–‡ä»¶ä½œä¸ºå¤‡ç”¨
    if not USE_DATABASE:
        logger.warning("âš ï¸ æ•°æ®åº“ä¸å¯ç”¨ï¼Œä½¿ç”¨JSONæ–‡ä»¶")
        try:
            if os.path.exists(STOCK_POOLS_CONFIG_FILE):
                with open(STOCK_POOLS_CONFIG_FILE, 'r', encoding='utf-8') as f:
                    data = _json.load(f)
                    logger.info(f"ğŸ“ ä»JSONæ–‡ä»¶åŠ è½½è‚¡ç¥¨æ± é…ç½®ï¼ŒåŒ…å«{len(data)}ä¸ªè‚¡ç¥¨æ± : {list(data.keys())}")
                    return data
            else:
                logger.warning(f"è‚¡ç¥¨æ± é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {STOCK_POOLS_CONFIG_FILE}")
        except _json.JSONDecodeError as e:
            logger.error(f"JSONè§£æé”™è¯¯: {e}ï¼Œæ–‡ä»¶å¯èƒ½æŸå")
        except PermissionError as e:
            logger.error(f"æ–‡ä»¶æƒé™é”™è¯¯: {e}")
        except Exception as e:
            logger.error(f"åŠ è½½è‚¡ç¥¨æ± é…ç½®å¤±è´¥: {e}")
    
    # åªæœ‰åœ¨å®Œå…¨å¤±è´¥æ—¶æ‰è¿”å›ç©ºå­—å…¸ï¼Œä¸å†ä½¿ç”¨é»˜è®¤é…ç½®
    logger.error("âŒ æ— æ³•åŠ è½½ä»»ä½•è‚¡ç¥¨æ± æ•°æ®ï¼è¯·æ£€æŸ¥æ•°æ®åº“æˆ–é…ç½®æ–‡ä»¶")
    return {}

def save_stock_pools(pools):
    """ä¿å­˜è‚¡ç¥¨æ± é…ç½®ï¼ˆæ•°æ®åº“ä¼˜å…ˆï¼‰"""
    
    # ä¼˜å…ˆä½¿ç”¨æ•°æ®åº“
    if USE_DATABASE:
        try:
            db = get_stock_pool_db()
            success_count = 0
            
            for pool_name, pool_data in pools.items():
                display_name = pool_data.get('name', pool_name)
                symbols = pool_data.get('symbols', [])
                description = pool_data.get('description', '')
                
                if db.save_pool(pool_name, display_name, symbols, description):
                    success_count += 1
            
            if success_count == len(pools):
                logger.info(f"âœ… æˆåŠŸä¿å­˜{success_count}ä¸ªè‚¡ç¥¨æ± åˆ°æ•°æ®åº“")
                
                # åŒæ—¶å¤‡ä»½åˆ°JSONæ–‡ä»¶
                try:
                    backup_file = f"{STOCK_POOLS_CONFIG_FILE}.db_backup"
                    db.backup_to_json(backup_file)
                    logger.info(f"ğŸ“ å·²åˆ›å»ºJSONå¤‡ä»½: {backup_file}")
                except Exception as e:
                    logger.warning(f"åˆ›å»ºJSONå¤‡ä»½å¤±è´¥: {e}")
                
                return True
            else:
                logger.error(f"âŒ åªæœ‰{success_count}/{len(pools)}ä¸ªè‚¡ç¥¨æ± ä¿å­˜æˆåŠŸ")
                return False
                
        except Exception as e:
            logger.error(f"âŒ æ•°æ®åº“ä¿å­˜å¤±è´¥: {e}ï¼Œå›é€€åˆ°JSONæ–‡ä»¶")
    
    # å›é€€åˆ°JSONæ–‡ä»¶ä¿å­˜
    import shutil
    
    try:
        # åˆ›å»ºå¤‡ä»½
        if os.path.exists(STOCK_POOLS_CONFIG_FILE):
            backup_file = f"{STOCK_POOLS_CONFIG_FILE}.backup"
            shutil.copy2(STOCK_POOLS_CONFIG_FILE, backup_file)
            logger.info(f"ğŸ“ å·²åˆ›å»ºJSONå¤‡ä»½: {backup_file}")
        
        # ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶è¿›è¡ŒåŸå­å†™å…¥
        temp_file = f"{STOCK_POOLS_CONFIG_FILE}.tmp"
        with open(temp_file, 'w', encoding='utf-8') as f:
            _json.dump(pools, f, ensure_ascii=False, indent=2)
        
        # åŸå­æ›¿æ¢
        os.replace(temp_file, STOCK_POOLS_CONFIG_FILE)
        
        # éªŒè¯å†™å…¥ç»“æœ
        with open(STOCK_POOLS_CONFIG_FILE, 'r', encoding='utf-8') as f:
            saved_data = _json.load(f)
            pool_count = len(saved_data)
        
        logger.info(f"ğŸ“ æˆåŠŸä¿å­˜è‚¡ç¥¨æ± é…ç½®åˆ°JSONæ–‡ä»¶ï¼ŒåŒ…å«{pool_count}ä¸ªè‚¡ç¥¨æ± ")
        return True
        
    except Exception as e:
        logger.error(f"JSONæ–‡ä»¶ä¿å­˜å¤±è´¥: {e}")
        # å¦‚æœæœ‰ä¸´æ—¶æ–‡ä»¶ï¼Œæ¸…ç†å®ƒ
        temp_file = f"{STOCK_POOLS_CONFIG_FILE}.tmp"
        if os.path.exists(temp_file):
            try:
                os.remove(temp_file)
            except:
                pass
        return False

@app.get("/api/strategy/stock_pools")
def get_predefined_stock_pools():
    """è·å–é¢„å®šä¹‰çš„è‚¡ç¥¨æ± """
    pools = load_stock_pools()
    return custom_json_response({"stock_pools": pools})


@app.post("/api/strategy/stock_pools")
def update_stock_pools(pools: Dict):
    """æ›´æ–°è‚¡ç¥¨æ± é…ç½®"""
    try:
        if save_stock_pools(pools.get("stock_pools", {})):
            return custom_json_response({"success": True, "message": "è‚¡ç¥¨æ± é…ç½®å·²æ›´æ–°"})
        else:
            return custom_json_response({"error": "ä¿å­˜é…ç½®å¤±è´¥"})
    except Exception as e:
        return custom_json_response({"error": f"æ›´æ–°è‚¡ç¥¨æ± å¤±è´¥: {str(e)}"})

@app.delete("/api/strategy/stock_pools/{pool_name}")
def delete_stock_pool(pool_name: str):
    """åˆ é™¤æŒ‡å®šçš„è‚¡ç¥¨æ± """
    try:
        pools = load_stock_pools()
        if pool_name in pools:
            del pools[pool_name]
            if save_stock_pools(pools):
                return custom_json_response({"success": True, "message": f"è‚¡ç¥¨æ±  {pool_name} å·²åˆ é™¤"})
        return custom_json_response({"error": "è‚¡ç¥¨æ± ä¸å­˜åœ¨æˆ–åˆ é™¤å¤±è´¥"})
    except Exception as e:
        return custom_json_response({"error": f"åˆ é™¤è‚¡ç¥¨æ± å¤±è´¥: {str(e)}"})

@app.get("/api/strategy/stock_pools_old")
def get_predefined_stock_pools_old():
    """è·å–é¢„å®šä¹‰çš„è‚¡ç¥¨æ± ï¼ˆæ—§ç‰ˆæœ¬ï¼Œä¿æŒå…¼å®¹ï¼‰"""
    pools = {
        "è“ç­¹è‚¡": {
            "name": "è“ç­¹è‚¡ç»„åˆ",
            "symbols": ["600519", "600000", "600036", "601398"],
            "description": "å¤§ç›˜è“ç­¹è‚¡ï¼Œç¨³å¥æŠ•èµ„"
        },
        "é“¶è¡Œè‚¡": {
            "name": "é“¶è¡Œæ¿å—",
            "symbols": ["600000", "600036", "601398", "601818", "601939"],
            "description": "é“¶è¡Œä¸šé¾™å¤´è‚¡ç¥¨"
        },
        "ç§‘æŠ€è‚¡": {
            "name": "ç§‘æŠ€æˆé•¿",
            "symbols": ["000002", "002594", "300750"],
            "description": "ç§‘æŠ€åˆ›æ–°æ¦‚å¿µè‚¡"
        },
        "å…¨å¸‚åœº": {
            "name": "å…¨å¸‚åœº",
            "symbols": ["600519", "600000", "600036", "601398", "601818", 
                       "000002", "002594", "300750", "000501"],
            "description": "å…¨éƒ¨å¯ç”¨è‚¡ç¥¨"
        }
    }
    
    return custom_json_response({"stock_pools": pools})

@app.post("/api/strategy/test_stock_pool")
def test_stock_pool(symbols: str):
    """æµ‹è¯•è‚¡ç¥¨æ± å¯ç”¨æ€§"""
    try:
        symbol_list = [s.strip() for s in symbols.split(',') if s.strip()]
        
        conn = sqlite3.connect("calibration.db")
        cursor = conn.cursor()
        
        results = []
        for symbol in symbol_list:
            cursor.execute("""
                SELECT COUNT(*) as count,
                       MIN(prediction_date) as first_date,
                       MAX(prediction_date) as last_date
                FROM predictions 
                WHERE symbol = ? AND actual_direction IS NOT NULL
            """, (symbol,))
            
            row = cursor.fetchone()
            count, first_date, last_date = row
            
            results.append({
                "symbol": symbol,
                "name": get_stock_name(symbol),
                "available": count > 0,
                "prediction_count": count,
                "date_range": f"{first_date} ~ {last_date}" if first_date else "æ— æ•°æ®"
            })
        
        conn.close()
        
        total_available = sum(1 for r in results if r["available"])
        
        return custom_json_response({
            "test_results": results,
            "summary": {
                "total_symbols": len(symbol_list),
                "available_symbols": total_available,
                "success_rate": f"{total_available/len(symbol_list)*100:.1f}%" if symbol_list else "0%"
            }
        })
        
    except Exception as e:
        return custom_json_response({"error": f"æµ‹è¯•è‚¡ç¥¨æ± å¤±è´¥: {str(e)}"})


# ==================== é«˜æ”¶ç›Šç­–ç•¥API ====================

@app.post("/api/strategy/high_return/run")
def run_high_return_strategy(
    symbols: str = None,  # é€—å·åˆ†éš”çš„è‚¡ç¥¨ä»£ç 
    days: int = 365,      # å›æµ‹å¤©æ•°
    config_override: dict = None,  # é…ç½®è¦†ç›–
    force_refresh: bool = False  # ğŸ”„ æ–°å¢ï¼šå¼ºåˆ¶åˆ·æ–°å†å²æ•°æ®
):
    """è¿è¡Œé«˜æ”¶ç›Šç­–ç•¥"""
    try:
        from backend.high_return_strategy import HighReturnStrategy
        
        # åˆ›å»ºç­–ç•¥å®ä¾‹
        strategy = HighReturnStrategy()
        
        # è§£æè‚¡ç¥¨ä»£ç 
        symbol_list = None
        if symbols:
            symbol_list = [s.strip() for s in symbols.split(',') if s.strip()]
        
        # é…ç½®è¦†ç›–
        if config_override:
            for key, value in config_override.items():
                if hasattr(strategy, key):
                    setattr(strategy, key, value)
        
        # è¿è¡Œå›æµ‹
        results = strategy.run_backtest(symbols=symbol_list, days=days, force_refresh=force_refresh)
        
        return custom_json_response({
            "success": True,
            "strategy_name": "é«˜æ”¶ç›Šç­–ç•¥",
            "results": results,
            "performance_rating": _get_performance_rating(results),
            "timestamp": _json.dumps(datetime.now(), default=str)
        })
        
    except Exception as e:
        import traceback
        return custom_json_response({
            "success": False,
            "error": f"é«˜æ”¶ç›Šç­–ç•¥æ‰§è¡Œå¤±è´¥: {str(e)}",
            "traceback": traceback.format_exc()
        })

@app.get("/api/strategy/high_return/config")
def get_high_return_config():
    """è·å–é«˜æ”¶ç›Šç­–ç•¥é…ç½®"""
    try:
        from backend.high_return_strategy import HighReturnStrategy
        
        strategy = HighReturnStrategy()
        config = strategy.config
        
        return custom_json_response({
            "success": True,
            "config": config,
            "strategy_description": config.get('strategy_description', ''),
            "performance_targets": config.get('performance_targets', {})
        })
        
    except Exception as e:
        return custom_json_response({
            "success": False,
            "error": f"è·å–é…ç½®å¤±è´¥: {str(e)}"
        })

@app.post("/api/strategy/high_return/update_config")
def update_high_return_config(config_updates: dict):
    """æ›´æ–°é«˜æ”¶ç›Šç­–ç•¥é…ç½®"""
    try:
        import yaml
        import os
        
        config_path = os.path.join(os.path.dirname(__file__), "..", "high_return_strategy_config.yaml")
        
        # è¯»å–ç°æœ‰é…ç½®
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        
        # æ›´æ–°é…ç½®
        def update_nested_dict(d, updates):
            for key, value in updates.items():
                if isinstance(value, dict) and key in d and isinstance(d[key], dict):
                    update_nested_dict(d[key], value)
                else:
                    d[key] = value
        
        update_nested_dict(config, config_updates)
        
        # ä¿å­˜é…ç½®
        with open(config_path, 'w', encoding='utf-8') as f:
            yaml.dump(config, f, ensure_ascii=False, indent=2)
        
        return custom_json_response({
            "success": True,
            "message": "é…ç½®æ›´æ–°æˆåŠŸ",
            "updated_config": config
        })
        
    except Exception as e:
        return custom_json_response({
            "success": False,
            "error": f"é…ç½®æ›´æ–°å¤±è´¥: {str(e)}"
        })

@app.get("/api/strategy/high_return/performance_analysis")
def get_high_return_performance_analysis(
    symbols: str = None,
    days: int = 365,
    benchmark: str = "000300"  # æ²ªæ·±300ä½œä¸ºåŸºå‡†
):
    """è·å–é«˜æ”¶ç›Šç­–ç•¥æ€§èƒ½åˆ†æ"""
    try:
        from backend.high_return_strategy import HighReturnStrategy
        
        # è¿è¡Œç­–ç•¥
        strategy = HighReturnStrategy()
        symbol_list = [s.strip() for s in symbols.split(',') if s.strip()] if symbols else None
        results = strategy.run_backtest(symbols=symbol_list, days=days)
        
        # è®¡ç®—é¢å¤–åˆ†ææŒ‡æ ‡
        analysis = {
            "basic_metrics": {
                "total_return": results.get('total_return', 0),
                "annualized_return": results.get('annualized_return', 0),
                "sharpe_ratio": results.get('sharpe_ratio', 0),
                "max_drawdown": results.get('max_drawdown', 0),
                "win_rate": results.get('win_rate', 0),
                "trade_count": results.get('trade_count', 0)
            },
            "advanced_metrics": _calculate_advanced_metrics(results),
            "risk_analysis": _analyze_risk_metrics(results),
            "performance_rating": _get_performance_rating(results),
            "comparison_vs_targets": _compare_vs_targets(results),
            "recommendations": _generate_recommendations(results)
        }
        
        return custom_json_response({
            "success": True,
            "analysis": analysis,
            "timestamp": _json.dumps(datetime.now(), default=str)
        })
        
    except Exception as e:
        return custom_json_response({
            "success": False,
            "error": f"æ€§èƒ½åˆ†æå¤±è´¥: {str(e)}"
        })

def _get_performance_rating(results: dict) -> dict:
    """è·å–ç­–ç•¥æ€§èƒ½è¯„çº§"""
    annual_return = results.get('annualized_return', 0)
    sharpe_ratio = results.get('sharpe_ratio', 0)
    max_drawdown = results.get('max_drawdown', 1)
    win_rate = results.get('win_rate', 0)
    
    # æ”¶ç›Šç‡è¯„çº§
    if annual_return > 0.30:
        return_rating = "ğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸ å“è¶Š"
        return_score = 5
    elif annual_return > 0.20:
        return_rating = "ğŸŒŸğŸŒŸğŸŒŸğŸŒŸ ä¼˜ç§€"
        return_score = 4
    elif annual_return > 0.10:
        return_rating = "ğŸŒŸğŸŒŸğŸŒŸ è‰¯å¥½"
        return_score = 3
    elif annual_return > 0.05:
        return_rating = "ğŸŒŸğŸŒŸ ä¸€èˆ¬"
        return_score = 2
    elif annual_return > 0:
        return_rating = "ğŸŒŸ åŠæ ¼"
        return_score = 1
    else:
        return_rating = "âŒ éœ€è¦æ”¹è¿›"
        return_score = 0
    
    # é£é™©è¯„çº§
    if sharpe_ratio > 2.0:
        risk_rating = "ğŸ›¡ï¸ğŸ›¡ï¸ğŸ›¡ï¸ ä¼˜ç§€"
        risk_score = 3
    elif sharpe_ratio > 1.0:
        risk_rating = "ğŸ›¡ï¸ğŸ›¡ï¸ è‰¯å¥½"
        risk_score = 2
    elif sharpe_ratio > 0.5:
        risk_rating = "ğŸ›¡ï¸ ä¸€èˆ¬"
        risk_score = 1
    else:
        risk_rating = "âš ï¸ è¾ƒå·®"
        risk_score = 0
    
    # ç»¼åˆè¯„çº§
    total_score = (return_score * 0.6 + risk_score * 0.4)
    if total_score >= 4.0:
        overall_rating = "ğŸ† å“è¶Šç­–ç•¥"
    elif total_score >= 3.0:
        overall_rating = "ğŸ¥‡ ä¼˜ç§€ç­–ç•¥"
    elif total_score >= 2.0:
        overall_rating = "ğŸ¥ˆ è‰¯å¥½ç­–ç•¥"
    elif total_score >= 1.0:
        overall_rating = "ğŸ¥‰ ä¸€èˆ¬ç­–ç•¥"
    else:
        overall_rating = "âŒ éœ€è¦æ”¹è¿›"
    
    return {
        "return_rating": return_rating,
        "risk_rating": risk_rating,
        "overall_rating": overall_rating,
        "total_score": total_score,
        "annual_return": annual_return,
        "sharpe_ratio": sharpe_ratio,
        "max_drawdown": max_drawdown,
        "win_rate": win_rate
    }

def _calculate_advanced_metrics(results: dict) -> dict:
    """è®¡ç®—é«˜çº§æŒ‡æ ‡"""
    daily_returns = results.get('daily_returns', [])
    trades = results.get('trades', [])
    
    if not daily_returns:
        return {}
    
    returns_array = np.array(daily_returns)
    
    # è®¡ç®—é«˜çº§æŒ‡æ ‡
    downside_returns = returns_array[returns_array < 0]
    downside_deviation = np.std(downside_returns) if len(downside_returns) > 0 else 0
    
    # Sortinoæ¯”ç‡
    if downside_deviation > 0:
        sortino_ratio = np.mean(returns_array) / downside_deviation * np.sqrt(252)
    else:
        sortino_ratio = 0
    
    # Calmaræ¯”ç‡
    max_drawdown = results.get('max_drawdown', 0.001)
    annualized_return = results.get('annualized_return', 0)
    calmar_ratio = annualized_return / max_drawdown if max_drawdown > 0 else 0
    
    # VaR (95%ç½®ä¿¡åº¦)
    var_95 = np.percentile(returns_array, 5) if len(returns_array) > 0 else 0
    
    # æœ€å¤§è¿ç»­äºæŸ
    max_consecutive_losses = 0
    current_losses = 0
    for trade in trades:
        if trade.get('return', 0) < 0:
            current_losses += 1
            max_consecutive_losses = max(max_consecutive_losses, current_losses)
        else:
            current_losses = 0
    
    return {
        "sortino_ratio": sortino_ratio,
        "calmar_ratio": calmar_ratio,
        "var_95": var_95,
        "downside_deviation": downside_deviation,
        "max_consecutive_losses": max_consecutive_losses,
        "profit_factor": _calculate_profit_factor(trades)
    }

def _calculate_profit_factor(trades: List[dict]) -> float:
    """è®¡ç®—ç›ˆäºæ¯”"""
    if not trades:
        return 0
    
    gross_profit = sum(t['return'] for t in trades if t['return'] > 0)
    gross_loss = abs(sum(t['return'] for t in trades if t['return'] < 0))
    
    return gross_profit / gross_loss if gross_loss > 0 else 0

def _analyze_risk_metrics(results: dict) -> dict:
    """åˆ†æé£é™©æŒ‡æ ‡"""
    max_drawdown = results.get('max_drawdown', 0)
    sharpe_ratio = results.get('sharpe_ratio', 0)
    win_rate = results.get('win_rate', 0)
    
    risk_level = "ä½"
    if max_drawdown > 0.15:
        risk_level = "é«˜"
    elif max_drawdown > 0.10:
        risk_level = "ä¸­"
    
    return {
        "risk_level": risk_level,
        "max_drawdown": max_drawdown,
        "volatility_estimate": abs(sharpe_ratio) * 0.16 if sharpe_ratio != 0 else 0,
        "win_rate": win_rate,
        "risk_assessment": "å¯æ¥å—" if max_drawdown < 0.12 else "éœ€è¦å…³æ³¨"
    }

def _compare_vs_targets(results: dict) -> dict:
    """ä¸ç›®æ ‡å¯¹æ¯”"""
    targets = {
        "annual_return": 0.30,
        "sharpe_ratio": 2.0,
        "max_drawdown": 0.10,
        "win_rate": 0.40
    }
    
    comparison = {}
    for metric, target in targets.items():
        actual = results.get(metric, 0)
        achievement = actual / target if target > 0 else 0
        status = "âœ… è¾¾æ ‡" if achievement >= 1.0 else "âš ï¸ æœªè¾¾æ ‡"
        
        comparison[metric] = {
            "target": target,
            "actual": actual,
            "achievement_rate": achievement,
            "status": status
        }
    
    return comparison

def _generate_recommendations(results: dict) -> List[str]:
    """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
    recommendations = []
    
    annual_return = results.get('annualized_return', 0)
    sharpe_ratio = results.get('sharpe_ratio', 0)
    max_drawdown = results.get('max_drawdown', 0)
    win_rate = results.get('win_rate', 0)
    
    if annual_return < 0.20:
        recommendations.append("ğŸ“ˆ å»ºè®®é™ä½ä¿¡å·é˜ˆå€¼ï¼Œå¢åŠ äº¤æ˜“æœºä¼š")
        recommendations.append("ğŸ”§ è€ƒè™‘ä¼˜åŒ–ä»“ä½ç®¡ç†ï¼Œæé«˜èµ„é‡‘åˆ©ç”¨ç‡")
    
    if sharpe_ratio < 1.5:
        recommendations.append("âš¡ å»ºè®®åŠ å¼ºé£é™©ç®¡ç†ï¼Œæé«˜å¤æ™®æ¯”ç‡")
        recommendations.append("ğŸ¯ ä¼˜åŒ–æ­¢ç›ˆæ­¢æŸç­–ç•¥")
    
    if max_drawdown > 0.12:
        recommendations.append("ğŸ›¡ï¸ å»ºè®®æ”¶ç´§æ­¢æŸï¼Œæ§åˆ¶æœ€å¤§å›æ’¤")
        recommendations.append("ğŸ“Š è€ƒè™‘é™ä½å•ç¬”ä»“ä½å¤§å°")
    
    if win_rate < 0.35:
        recommendations.append("ğŸ² å»ºè®®å¢åŠ ä¿¡å·ç¡®è®¤æœºåˆ¶ï¼Œæé«˜èƒœç‡")
        recommendations.append("ğŸ” ä¼˜åŒ–å…¥åœºæ—¶æœºé€‰æ‹©")
    
    if not recommendations:
        recommendations.append("ğŸ‰ ç­–ç•¥è¡¨ç°ä¼˜ç§€ï¼Œå»ºè®®ä¿æŒå½“å‰é…ç½®")
        recommendations.append("ğŸ“ˆ å¯è€ƒè™‘é€‚åº¦å¢åŠ ä»“ä½æˆ–æ‰©å±•è‚¡ç¥¨æ± ")
    
    return recommendations

@app.post("/api/opening_prediction")
async def get_opening_prediction(request: Dict[str, Any]):
    """è·å–å¼€ç›˜é¢„æµ‹ç­–ç•¥"""
    try:
        from backend.opening_prediction import OpeningPredictor, format_prediction_result
        
        # è·å–è‚¡ç¥¨æ± 
        stock_pool_input = request.get('stock_pool', [])
        
        # è·å–å¼ºåˆ¶åˆ·æ–°å‚æ•°
        force_refresh = request.get('force_refresh', False)
        
        # å¤„ç†ä¸åŒç±»å‹çš„è¾“å…¥
        if isinstance(stock_pool_input, str):
            # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œå°è¯•ä»è‚¡ç¥¨æ± é…ç½®ä¸­è·å–
            logger.info(f"æ”¶åˆ°è‚¡ç¥¨æ± åç§°: {stock_pool_input}")
            try:
                # ä½¿ç”¨ç»Ÿä¸€çš„è‚¡ç¥¨æ± åŠ è½½å‡½æ•°
                pools_config = load_stock_pools()
                
                # æŸ¥æ‰¾åŒ¹é…çš„è‚¡ç¥¨æ± 
                stock_pool = None
                logger.info(f"åœ¨é…ç½®ä¸­æŸ¥æ‰¾è‚¡ç¥¨æ± : '{stock_pool_input}'")
                for pool_name, pool_data in pools_config.items():
                    logger.info(f"æ£€æŸ¥è‚¡ç¥¨æ± : '{pool_name}' (åŒ¹é…: {pool_name == stock_pool_input})")
                    if pool_name == stock_pool_input or pool_data.get('name') == stock_pool_input:
                        stock_pool = pool_data.get('symbols', [])
                        logger.info(f"æ‰¾åˆ°åŒ¹é…çš„è‚¡ç¥¨æ± : {stock_pool}")
                        break
                
                if not stock_pool:
                    # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œå°è¯•ä½¿ç”¨"æˆ‘çš„è‡ªé€‰"è‚¡ç¥¨æ± 
                    if 'æˆ‘çš„è‡ªé€‰' in pools_config:
                        stock_pool = pools_config['æˆ‘çš„è‡ªé€‰'].get('symbols', [])
                        logger.info(f"ä½¿ç”¨é»˜è®¤'æˆ‘çš„è‡ªé€‰'è‚¡ç¥¨æ± : {stock_pool}")
                    else:
                        # å¦‚æœè¿"æˆ‘çš„è‡ªé€‰"éƒ½æ²¡æœ‰ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨çš„è‚¡ç¥¨æ± 
                        if pools_config:
                            first_pool_name = list(pools_config.keys())[0]
                            stock_pool = pools_config[first_pool_name].get('symbols', [])
                            logger.info(f"ä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨è‚¡ç¥¨æ± '{first_pool_name}': {stock_pool}")
                        else:
                            stock_pool = []
                            logger.error("æ²¡æœ‰å¯ç”¨çš„è‚¡ç¥¨æ± æ•°æ®")
                        
            except Exception as e:
                logger.error(f"åŠ è½½è‚¡ç¥¨æ± é…ç½®å¤±è´¥: {e}")
                stock_pool = []
                
        elif isinstance(stock_pool_input, list):
            # å¦‚æœæ˜¯åˆ—è¡¨ï¼Œç›´æ¥ä½¿ç”¨
            stock_pool = stock_pool_input
        else:
            # å…¶ä»–æƒ…å†µä½¿ç”¨é»˜è®¤
            stock_pool = ["000501", "000519", "002182", "600176", "600585", "002436", "600710"]
        
        logger.info(f"å¼€å§‹ç”Ÿæˆå¼€ç›˜é¢„æµ‹ï¼Œè‚¡ç¥¨æ± : {stock_pool}")
        logger.info(f"è‚¡ç¥¨æ± ç±»å‹: {type(stock_pool)}, é•¿åº¦: {len(stock_pool)}")
        
        # ç¡®ä¿stock_poolæ˜¯åˆ—è¡¨ä¸”ä¸ä¸ºç©º
        if not isinstance(stock_pool, list) or not stock_pool:
            logger.error(f"è‚¡ç¥¨æ± æ ¼å¼é”™è¯¯: {stock_pool}")
            raise ValueError("è‚¡ç¥¨æ± å¿…é¡»æ˜¯éç©ºçš„è‚¡ç¥¨ä»£ç åˆ—è¡¨")
        
        # è·å–å½“å‰æŒä»“ä¿¡æ¯
        current_positions = []
        try:
            portfolio = portfolio_manager.get_portfolio()
            current_positions = [
                {
                    'symbol': pos.symbol,
                    'name': pos.name,
                    'quantity': pos.quantity,
                    'avg_cost': pos.avg_cost,
                    'current_price': pos.current_price,
                    'market_value': pos.market_value,
                    'profit_loss': pos.profit_loss,
                    'profit_loss_pct': pos.profit_loss_pct
                }
                for pos in portfolio.positions
            ]
            logger.info(f"è·å–åˆ°å½“å‰æŒä»“ {len(current_positions)} åªè‚¡ç¥¨")
        except Exception as e:
            logger.warning(f"è·å–æŒä»“ä¿¡æ¯å¤±è´¥ï¼Œå°†ä¸è€ƒè™‘æŒä»“: {e}")
        
        # åˆ›å»ºé¢„æµ‹å™¨
        predictor = OpeningPredictor()
        
        # ç”Ÿæˆé¢„æµ‹ç»“æœï¼ˆé›†æˆæŒä»“ä¿¡æ¯ï¼‰
        prediction_result = predictor.predict_opening_strategy(stock_pool, current_positions, force_refresh_history=force_refresh)

        # è‡ªåŠ¨è”åŠ¨ï¼šæ ¹æ®é¢„æµ‹ä¸å›æµ‹é£æ ¼å»ºè®®ï¼Œç”Ÿæˆâ€œå¾…æ‰§è¡Œâ€çš„äº¤æ˜“æ¸…å•ï¼ˆä¸ç›´æ¥ä¸‹å•ï¼‰
        try:
            planned_trades = []
            for p in prediction_result.stock_predictions:
                # åªåšå¤šé€»è¾‘ï¼šå½“recommendation==buyæ—¶ï¼Œå»ºè®®ä¹°å…¥ï¼›å½“should_sellæœåŠ¡ç«¯è¯„ä¼°ä¸ºçœŸæ—¶ï¼Œå»ºè®®å–å‡º
                symbol = p.symbol
                name = p.name
                # ä¹°å…¥å»ºè®®ï¼šæŒ‰Kellyå»ºè®®æ•°é‡ä¼°ç®—ï¼ˆè‹¥å‰é¢å·²ç½®é›¶ï¼Œåˆ™ä¸ä¼šäº§ç”Ÿæç«¯ä»“ä½ï¼‰
                if p.recommendation == 'buy' and p.current_price > 0:
                    # è®¡ç®—æœ€å¤§å¯ä¹°ï¼ˆç»“åˆç°é‡‘ç®¡ç†ï¼‰
                    max_qty = portfolio_manager.calculate_max_buy_quantity(symbol, p.current_price)
                    # å»ºè®®ä¹°å…¥æ•°é‡ï¼šä¸è¶…è¿‡æœ€å¤§å¯ä¹°ï¼Œä¸”ä¸è¶…è¿‡2000
                    buy_qty = min(2000, max(0, max_qty))
                    if buy_qty >= 100:
                        planned_trades.append({
                            'action': 'buy',
                            'symbol': symbol,
                            'name': name,
                            'price': p.current_price,
                            'quantity': buy_qty,
                            'note': 'opening_prediction_planned'
                        })
                # å–å‡ºå»ºè®®ï¼šè°ƒç”¨ç°æœ‰é£æ§åˆ¤å®šï¼ˆä»…æŒä»“æ—¶ï¼‰
                holding = predictor._get_holding_info(symbol, current_positions)
                if holding.get('is_holding') and p.current_price > 0:
                    if predictor._should_sell_backtest_style(p, holding):
                        sell_qty = holding.get('quantity', 0)
                        if sell_qty > 0:
                            planned_trades.append({
                                'action': 'sell',
                                'symbol': symbol,
                                'name': name,
                                'price': p.current_price,
                                'quantity': sell_qty,
                                'note': 'opening_prediction_planned'
                            })
        except Exception as _e:
            planned_trades = []
        
        # æ ¼å¼åŒ–ç»“æœ
        formatted_result = format_prediction_result(prediction_result)
        
        return {
            "success": True,
            "data": formatted_result,
            "planned_trades": planned_trades
        }
        
    except Exception as e:
        logger.error(f"ç”Ÿæˆå¼€ç›˜é¢„æµ‹å¤±è´¥: {e}")
        logger.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
        return {
            "success": False,
            "error": str(e),
            "details": traceback.format_exc()
        }


# ==================== æŒä»“ç®¡ç†API ====================

@app.get("/api/portfolio")
async def get_portfolio():
    """è·å–æŠ•èµ„ç»„åˆæ¦‚å†µ"""
    try:
        portfolio = portfolio_manager.get_portfolio()
        
        # è½¬æ¢ä¸ºå“åº”æ¨¡å‹
        positions_response = [
            {
                "symbol": pos.symbol,
                "name": pos.name,
                "quantity": pos.quantity,
                "avg_cost": pos.avg_cost,
                "current_price": pos.current_price,
                "market_value": pos.market_value,
                "profit_loss": pos.profit_loss,
                "profit_loss_pct": pos.profit_loss_pct,
                "last_updated": pos.last_updated
            }
            for pos in portfolio.positions
        ]
        
        return custom_json_response({
            "success": True,
            "data": {
                "total_assets": portfolio.total_assets,
                "available_cash": portfolio.available_cash,
                "market_value": portfolio.market_value,
                "total_profit_loss": portfolio.total_profit_loss,
                "total_profit_loss_pct": portfolio.total_profit_loss_pct,
                "positions": positions_response,
                "position_count": portfolio.position_count
            }
        })
        
    except Exception as e:
        logger.error(f"è·å–æŠ•èµ„ç»„åˆå¤±è´¥: {e}")
        return custom_json_response({
            "success": False,
            "error": f"è·å–æŠ•èµ„ç»„åˆå¤±è´¥: {str(e)}"
        })

@app.post("/api/portfolio/transaction")
async def add_transaction(request: TransactionRequest):
    """æ·»åŠ äº¤æ˜“è®°å½•"""
    try:
        success = portfolio_manager.add_transaction(
            symbol=request.symbol,
            name=request.name,
            action=request.action,
            quantity=request.quantity,
            price=request.price,
            fee=request.fee,
            note=request.note
        )
        
        if success:
            return custom_json_response({
                "success": True,
                "message": f"äº¤æ˜“è®°å½•æ·»åŠ æˆåŠŸ: {request.action} {request.quantity}è‚¡ {request.symbol}"
            })
        else:
            return custom_json_response({
                "success": False,
                "error": "äº¤æ˜“è®°å½•æ·»åŠ å¤±è´¥"
            })
            
    except Exception as e:
        logger.error(f"æ·»åŠ äº¤æ˜“è®°å½•å¤±è´¥: {e}")
        return custom_json_response({
            "success": False,
            "error": f"æ·»åŠ äº¤æ˜“è®°å½•å¤±è´¥: {str(e)}"
        })

@app.get("/api/portfolio/positions")
async def get_positions():
    """è·å–æ‰€æœ‰æŒä»“"""
    try:
        positions = portfolio_manager.get_positions()
        
        positions_response = [
            {
                "symbol": pos.symbol,
                "name": pos.name,
                "quantity": pos.quantity,
                "avg_cost": pos.avg_cost,
                "current_price": pos.current_price,
                "market_value": pos.market_value,
                "profit_loss": pos.profit_loss,
                "profit_loss_pct": pos.profit_loss_pct,
                "last_updated": pos.last_updated
            }
            for pos in positions
        ]
        
        return custom_json_response({
            "success": True,
            "data": positions_response
        })
        
    except Exception as e:
        logger.error(f"è·å–æŒä»“å¤±è´¥: {e}")
        return custom_json_response({
            "success": False,
            "error": f"è·å–æŒä»“å¤±è´¥: {str(e)}"
        })

@app.get("/api/portfolio/transactions")
async def get_transactions(limit: int = Query(50), symbol: str = Query(None)):
    """è·å–äº¤æ˜“è®°å½•"""
    try:
        transactions = portfolio_manager.get_transactions(limit=limit, symbol=symbol)
        
        return custom_json_response({
            "success": True,
            "data": [
                {
                    "id": t.id,
                    "symbol": t.symbol,
                    "name": t.name,
                    "action": t.action,
                    "quantity": t.quantity,
                    "price": t.price,
                    "amount": t.amount,
                    "fee": t.fee,
                    "timestamp": t.timestamp,
                    "note": t.note
                }
                for t in transactions
            ]
        })
        
    except Exception as e:
        logger.error(f"è·å–äº¤æ˜“è®°å½•å¤±è´¥: {e}")
        return custom_json_response({
            "success": False,
            "error": f"è·å–äº¤æ˜“è®°å½•å¤±è´¥: {str(e)}"
        })

@app.get("/api/portfolio/cash")
async def get_available_cash():
    """è·å–å¯ç”¨èµ„é‡‘"""
    try:
        cash = portfolio_manager.get_available_cash()
        return custom_json_response({
            "success": True,
            "data": {"available_cash": cash}
        })
        
    except Exception as e:
        logger.error(f"è·å–å¯ç”¨èµ„é‡‘å¤±è´¥: {e}")
        return custom_json_response({
            "success": False,
            "error": f"è·å–å¯ç”¨èµ„é‡‘å¤±è´¥: {str(e)}"
        })

@app.post("/api/portfolio/cash")
async def update_cash(request: Dict[str, Any]):
    """æ›´æ–°å¯ç”¨èµ„é‡‘"""
    try:
        cash = request.get("amount", 0)
        portfolio_manager.update_cash(cash)
        return custom_json_response({
            "success": True,
            "message": f"èµ„é‡‘æ›´æ–°æˆåŠŸ: Â¥{cash:.2f}"
        })
        
    except Exception as e:
        logger.error(f"æ›´æ–°èµ„é‡‘å¤±è´¥: {e}")
        return custom_json_response({
            "success": False,
            "error": f"æ›´æ–°èµ„é‡‘å¤±è´¥: {str(e)}"
        })

@app.get("/api/portfolio/max_buy")
async def get_max_buy_quantity(symbol: str = Query(...), price: float = Query(...)):
    """è®¡ç®—æœ€å¤§å¯ä¹°æ•°é‡"""
    try:
        max_quantity = portfolio_manager.calculate_max_buy_quantity(symbol, price)
        return custom_json_response({
            "success": True,
            "data": {
                "symbol": symbol,
                "price": price,
                "max_quantity": max_quantity,
                "max_amount": max_quantity * price
            }
        })
        
    except Exception as e:
        logger.error(f"è®¡ç®—æœ€å¤§ä¹°å…¥æ•°é‡å¤±è´¥: {e}")
        return custom_json_response({
            "success": False,
            "error": f"è®¡ç®—æœ€å¤§ä¹°å…¥æ•°é‡å¤±è´¥: {str(e)}"
        })

@app.put("/api/portfolio/position")
async def edit_position(request: PositionEditRequest):
    """ç¼–è¾‘æŒä»“ä¿¡æ¯"""
    try:
        success = portfolio_manager.edit_position(
            symbol=request.symbol,
            new_quantity=request.quantity,
            new_avg_cost=request.avg_cost,
            note=request.note
        )
        
        if success:
            return custom_json_response({
                "success": True,
                "message": f"æŒä»“ç¼–è¾‘æˆåŠŸ: {request.symbol}"
            })
        else:
            return custom_json_response({
                "success": False,
                "error": "æŒä»“ç¼–è¾‘å¤±è´¥"
            })
            
    except Exception as e:
        logger.error(f"ç¼–è¾‘æŒä»“å¤±è´¥: {e}")
        return custom_json_response({
            "success": False,
            "error": f"ç¼–è¾‘æŒä»“å¤±è´¥: {str(e)}"
        })

@app.delete("/api/portfolio/position/{symbol}")
async def delete_position(symbol: str):
    """åˆ é™¤æŒä»“"""
    try:
        success = portfolio_manager.delete_position(symbol)
        
        if success:
            return custom_json_response({
                "success": True,
                "message": f"æŒä»“åˆ é™¤æˆåŠŸ: {symbol}"
            })
        else:
            return custom_json_response({
                "success": False,
                "error": "æŒä»“åˆ é™¤å¤±è´¥"
            })
            
    except Exception as e:
        logger.error(f"åˆ é™¤æŒä»“å¤±è´¥: {e}")
        return custom_json_response({
            "success": False,
            "error": f"åˆ é™¤æŒä»“å¤±è´¥: {str(e)}"
        })

@app.post("/api/portfolio/batch_transactions")
async def batch_add_transactions(request: BatchTransactionRequest):
    """æ‰¹é‡æ·»åŠ äº¤æ˜“è®°å½•"""
    try:
        transactions_data = [
            {
                'symbol': t.symbol,
                'name': t.name,
                'action': t.action,
                'quantity': t.quantity,
                'price': t.price,
                'fee': t.fee,
                'note': t.note
            }
            for t in request.transactions
        ]
        
        results = portfolio_manager.batch_add_transactions(transactions_data)
        
        return custom_json_response({
            "success": True,
            "data": results
        })
        
    except Exception as e:
        logger.error(f"æ‰¹é‡æ·»åŠ äº¤æ˜“è®°å½•å¤±è´¥: {e}")
        return custom_json_response({
            "success": False,
            "error": f"æ‰¹é‡æ·»åŠ äº¤æ˜“è®°å½•å¤±è´¥: {str(e)}"
        })

@app.get("/api/portfolio/analysis")
async def get_portfolio_analysis():
    """è·å–æŒä»“åˆ†æ"""
    try:
        analysis = portfolio_manager.get_position_analysis()
        return custom_json_response({
            "success": True,
            "data": analysis
        })
        
    except Exception as e:
        logger.error(f"è·å–æŒä»“åˆ†æå¤±è´¥: {e}")
        return custom_json_response({
            "success": False,
            "error": f"è·å–æŒä»“åˆ†æå¤±è´¥: {str(e)}"
        })

@app.post("/api/portfolio/sync_stock_pool")
async def sync_with_stock_pool(stock_pool_symbols: List[str]):
    """ä¸è‚¡ç¥¨æ± åŒæ­¥åˆ†æ"""
    try:
        sync_result = portfolio_manager.sync_with_stock_pool(stock_pool_symbols)
        return custom_json_response({
            "success": True,
            "data": sync_result
        })
        
    except Exception as e:
        logger.error(f"è‚¡ç¥¨æ± åŒæ­¥åˆ†æå¤±è´¥: {e}")
        return custom_json_response({
            "success": False,
            "error": f"è‚¡ç¥¨æ± åŒæ­¥åˆ†æå¤±è´¥: {str(e)}"
        })

@app.post("/api/portfolio/fix_stock_names")
async def fix_stock_names():
    """ä¿®å¤è‚¡ç¥¨åç§°"""
    try:
        result = portfolio_manager.fix_stock_names()
        return custom_json_response(result)
    except Exception as e:
        logger.error(f"ä¿®å¤è‚¡ç¥¨åç§°å¤±è´¥: {e}")
        return custom_json_response({
            "success": False,
            "error": f"ä¿®å¤è‚¡ç¥¨åç§°å¤±è´¥: {str(e)}"
        })

# å¥åº·æ£€æŸ¥ç«¯ç‚¹
@app.get("/")
async def root():
    """æ ¹è·¯å¾„å¥åº·æ£€æŸ¥"""
    return {
        "message": "Auto-GPT-Stock API is running",
        "version": "0.1.0",
        "environment": ENVIRONMENT,
        "status": "healthy"
    }

@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "environment": ENVIRONMENT
    }

# å¯åŠ¨é…ç½®
if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        "app:app",
        host="0.0.0.0",
        port=PORT,
        reload=ENVIRONMENT == "development"
    )


