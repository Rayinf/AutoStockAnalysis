#!/usr/bin/env python3
"""
è¯¦ç»†æ¼”ç¤ºäº¤å‰éªŒè¯çš„æ‰§è¡Œè¿‡ç¨‹
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

import pandas as pd
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.isotonic import IsotonicRegression
from sklearn.metrics import brier_score_loss
import sqlite3
# import matplotlib.pyplot as plt  # ç§»é™¤matplotlibä¾èµ–

def detailed_cross_validation_demo():
    """è¯¦ç»†æ¼”ç¤ºäº¤å‰éªŒè¯è¿‡ç¨‹"""
    print("ğŸ” è¯¦ç»†äº¤å‰éªŒè¯æ¼”ç¤º")
    print("=" * 60)
    
    # 1. è·å–æ•°æ®
    print("\nğŸ“Š æ­¥éª¤1: è·å–æ‰€æœ‰é¢„æµ‹æ•°æ®")
    conn = sqlite3.connect("calibration.db")
    query = """
        SELECT prediction_date, symbol, predicted_probability, actual_direction
        FROM predictions 
        WHERE actual_direction IS NOT NULL
        ORDER BY prediction_date
    """
    df = pd.read_sql_query(query, conn)
    conn.close()
    
    print(f"æ€»æ•°æ®é‡: {len(df)} æ¡è®°å½•")
    print(f"æ—¶é—´èŒƒå›´: {df['prediction_date'].min()} ~ {df['prediction_date'].max()}")
    print(f"è‚¡ç¥¨æ•°é‡: {df['symbol'].nunique()} åª")
    
    # 2. è®¾ç½®äº¤å‰éªŒè¯å‚æ•°
    n_folds = 5
    fold_size = len(df) // n_folds
    print(f"\nğŸ”„ æ­¥éª¤2: è®¾ç½®äº¤å‰éªŒè¯å‚æ•°")
    print(f"æŠ˜æ•°: {n_folds}")
    print(f"æ¯æŠ˜å¤§å°: {fold_size} æ¡è®°å½•")
    
    # 3. æ‰§è¡Œäº¤å‰éªŒè¯
    print(f"\nâš™ï¸ æ­¥éª¤3: æ‰§è¡Œäº¤å‰éªŒè¯")
    print("-" * 60)
    
    platt_improvements = []
    isotonic_improvements = []
    fold_details = []
    
    for fold in range(n_folds):
        print(f"\nğŸ“‹ Fold {fold + 1}/{n_folds}")
        
        # è®¡ç®—æ•°æ®åˆ†å‰²ç‚¹
        train_end = fold * fold_size
        test_start = fold * fold_size
        test_end = (fold + 1) * fold_size
        
        if train_end < 50:  # è®­ç»ƒé›†å¤ªå°ï¼Œè·³è¿‡
            print("   âš ï¸ è®­ç»ƒé›†å¤ªå°ï¼Œè·³è¿‡")
            continue
            
        # åˆ†å‰²æ•°æ®
        train_df = df.iloc[:train_end]
        test_df = df.iloc[test_start:test_end]
        
        if len(test_df) == 0:
            print("   âš ï¸ æµ‹è¯•é›†ä¸ºç©ºï¼Œè·³è¿‡")
            continue
        
        print(f"   è®­ç»ƒé›†: {len(train_df)} æ¡ ({train_df['prediction_date'].min()} ~ {train_df['prediction_date'].max()})")
        print(f"   æµ‹è¯•é›†: {len(test_df)} æ¡ ({test_df['prediction_date'].min()} ~ {test_df['prediction_date'].max()})")
        
        # å‡†å¤‡æ•°æ®
        train_probs = train_df['predicted_probability'].values
        train_labels = train_df['actual_direction'].values
        test_probs = test_df['predicted_probability'].values
        test_labels = test_df['actual_direction'].values
        
        # æµ‹è¯•Platt Scaling
        try:
            # è®­ç»ƒæ ¡å‡†æ¨¡å‹
            epsilon = 1e-7
            train_probs_clipped = np.clip(train_probs, epsilon, 1 - epsilon)
            train_logits = np.log(train_probs_clipped / (1 - train_probs_clipped))
            
            platt_model = LogisticRegression()
            platt_model.fit(train_logits.reshape(-1, 1), train_labels)
            
            # åœ¨æµ‹è¯•é›†ä¸Šåº”ç”¨æ ¡å‡†
            test_probs_clipped = np.clip(test_probs, epsilon, 1 - epsilon)
            test_logits = np.log(test_probs_clipped / (1 - test_probs_clipped))
            calibrated_test_probs = platt_model.predict_proba(test_logits.reshape(-1, 1))[:, 1]
            
            # è®¡ç®—æ”¹è¿›
            original_brier = brier_score_loss(test_labels, test_probs)
            calibrated_brier = brier_score_loss(test_labels, calibrated_test_probs)
            platt_improvement = (original_brier - calibrated_brier) / original_brier * 100
            
            platt_improvements.append(platt_improvement)
            print(f"   Platt Scalingæ”¹è¿›: {platt_improvement:.2f}%")
            
        except Exception as e:
            print(f"   Platt Scalingå¤±è´¥: {e}")
            platt_improvement = 0
        
        # æµ‹è¯•Isotonic Regression
        try:
            isotonic_model = IsotonicRegression(out_of_bounds='clip')
            isotonic_model.fit(train_probs, train_labels)
            
            calibrated_test_probs_iso = isotonic_model.transform(test_probs)
            
            original_brier = brier_score_loss(test_labels, test_probs)
            calibrated_brier_iso = brier_score_loss(test_labels, calibrated_test_probs_iso)
            isotonic_improvement = (original_brier - calibrated_brier_iso) / original_brier * 100
            
            isotonic_improvements.append(isotonic_improvement)
            print(f"   Isotonic Regressionæ”¹è¿›: {isotonic_improvement:.2f}%")
            
        except Exception as e:
            print(f"   Isotonic Regressionå¤±è´¥: {e}")
            isotonic_improvement = 0
        
        # ä¿å­˜è¯¦æƒ…
        fold_details.append({
            'fold': fold + 1,
            'train_size': len(train_df),
            'test_size': len(test_df),
            'train_period': f"{train_df['prediction_date'].min()} ~ {train_df['prediction_date'].max()}",
            'test_period': f"{test_df['prediction_date'].min()} ~ {test_df['prediction_date'].max()}",
            'platt_improvement': platt_improvement,
            'isotonic_improvement': isotonic_improvement
        })
    
    # 4. æ±‡æ€»ç»“æœ
    print(f"\nğŸ“Š æ­¥éª¤4: æ±‡æ€»äº¤å‰éªŒè¯ç»“æœ")
    print("-" * 60)
    
    if platt_improvements:
        platt_mean = np.mean(platt_improvements)
        platt_std = np.std(platt_improvements)
        print(f"Platt Scaling:")
        print(f"  å¹³å‡æ”¹è¿›: {platt_mean:.2f}% Â± {platt_std:.2f}%")
        print(f"  æ”¹è¿›èŒƒå›´: {min(platt_improvements):.2f}% ~ {max(platt_improvements):.2f}%")
        print(f"  æœ‰æ•ˆæŠ˜æ•°: {len(platt_improvements)}")
    
    if isotonic_improvements:
        isotonic_mean = np.mean(isotonic_improvements)
        isotonic_std = np.std(isotonic_improvements)
        print(f"Isotonic Regression:")
        print(f"  å¹³å‡æ”¹è¿›: {isotonic_mean:.2f}% Â± {isotonic_std:.2f}%")
        print(f"  æ”¹è¿›èŒƒå›´: {min(isotonic_improvements):.2f}% ~ {max(isotonic_improvements):.2f}%")
        print(f"  æœ‰æ•ˆæŠ˜æ•°: {len(isotonic_improvements)}")
    
    # 5. è¯¦ç»†ç»“æœè¡¨
    print(f"\nğŸ“‹ æ­¥éª¤5: å„æŠ˜è¯¦ç»†ç»“æœ")
    print("-" * 80)
    print(f"{'Fold':<4} {'è®­ç»ƒé›†':<8} {'æµ‹è¯•é›†':<8} {'Plattæ”¹è¿›':<10} {'Isotonicæ”¹è¿›':<12} {'æµ‹è¯•æœŸé—´'}")
    print("-" * 80)
    
    for detail in fold_details:
        print(f"{detail['fold']:<4} {detail['train_size']:<8} {detail['test_size']:<8} "
              f"{detail['platt_improvement']:<10.2f} {detail['isotonic_improvement']:<12.2f} "
              f"{detail['test_period']}")
    
    # 6. è§£é‡Šäº¤å‰éªŒè¯çš„ä¼˜åŠ¿
    print(f"\nğŸ’¡ äº¤å‰éªŒè¯çš„ä¼˜åŠ¿")
    print("-" * 40)
    print("1. æ—¶é—´åºåˆ—åˆ†å‰²: ä¸¥æ ¼æŒ‰æ—¶é—´é¡ºåºï¼Œé¿å…æœªæ¥æ•°æ®æ³„éœ²")
    print("2. å¤šæ¬¡éªŒè¯: æ¯ä¸ªæ—¶é—´æ®µéƒ½ä½œä¸ºæµ‹è¯•é›†ï¼Œç»“æœæ›´å¯é ")
    print("3. ç»Ÿè®¡æ„ä¹‰: æä¾›å‡å€¼å’Œæ ‡å‡†å·®ï¼Œè¯„ä¼°ç¨³å®šæ€§")
    print("4. æ¸è¿›è®­ç»ƒ: æ¨¡æ‹ŸçœŸå®åœºæ™¯ï¼Œè®­ç»ƒé›†é€æ¸å¢å¤§")
    
    return {
        'platt_improvements': platt_improvements,
        'isotonic_improvements': isotonic_improvements,
        'fold_details': fold_details
    }

def visualize_cross_validation():
    """å¯è§†åŒ–äº¤å‰éªŒè¯è¿‡ç¨‹"""
    print("\nğŸ¨ å¯è§†åŒ–äº¤å‰éªŒè¯åˆ†å‰²")
    
    # è·å–æ•°æ®æ—¶é—´èŒƒå›´
    conn = sqlite3.connect("calibration.db")
    query = "SELECT prediction_date FROM predictions WHERE actual_direction IS NOT NULL ORDER BY prediction_date"
    dates_df = pd.read_sql_query(query, conn)
    conn.close()
    
    dates = pd.to_datetime(dates_df['prediction_date'])
    total_days = (dates.max() - dates.min()).days
    
    print(f"æ•°æ®æ—¶é—´è·¨åº¦: {total_days} å¤©")
    print(f"ä» {dates.min().date()} åˆ° {dates.max().date()}")
    
    # æ˜¾ç¤ºåˆ†å‰²ç¤ºæ„å›¾
    n_folds = 5
    fold_size = len(dates) // n_folds
    
    print(f"\nğŸ“… 5æŠ˜äº¤å‰éªŒè¯åˆ†å‰²ç¤ºæ„:")
    print("=" * 60)
    
    for fold in range(n_folds):
        train_end = fold * fold_size
        test_start = fold * fold_size
        test_end = (fold + 1) * fold_size
        
        if train_end < 50:
            continue
            
        train_dates = dates.iloc[:train_end]
        test_dates = dates.iloc[test_start:test_end]
        
        print(f"Fold {fold + 1}:")
        print(f"  è®­ç»ƒ: {train_dates.min().date()} ~ {train_dates.max().date()} ({len(train_dates)} æ¡)")
        print(f"  æµ‹è¯•: {test_dates.min().date()} ~ {test_dates.max().date()} ({len(test_dates)} æ¡)")
        print(f"  {'â–ˆ' * (len(train_dates) // 100)}{'â–‘' * (len(test_dates) // 100)}")
        print()

if __name__ == "__main__":
    # æ‰§è¡Œè¯¦ç»†æ¼”ç¤º
    results = detailed_cross_validation_demo()
    
    # å¯è§†åŒ–åˆ†å‰²è¿‡ç¨‹
    visualize_cross_validation()
    
    print("\nğŸ¯ æ€»ç»“:")
    print("äº¤å‰éªŒè¯é€šè¿‡æ—¶é—´åºåˆ—åˆ†å‰²ç¡®ä¿äº†:")
    print("- è®­ç»ƒé›†å§‹ç»ˆåœ¨æµ‹è¯•é›†ä¹‹å‰ï¼ˆæ—¶é—´é¡ºåºï¼‰")
    print("- æ¯æŠ˜éƒ½ç‹¬ç«‹æµ‹è¯•æ ¡å‡†æ•ˆæœ")
    print("- æä¾›ç¨³å¥çš„æ€§èƒ½ä¼°è®¡")
    print("- é¿å…è¿‡æ‹Ÿåˆå’Œæ•°æ®æ³„éœ²")

è¯¦ç»†æ¼”ç¤ºäº¤å‰éªŒè¯çš„æ‰§è¡Œè¿‡ç¨‹
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

import pandas as pd
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.isotonic import IsotonicRegression
from sklearn.metrics import brier_score_loss
import sqlite3
# import matplotlib.pyplot as plt  # ç§»é™¤matplotlibä¾èµ–

def detailed_cross_validation_demo():
    """è¯¦ç»†æ¼”ç¤ºäº¤å‰éªŒè¯è¿‡ç¨‹"""
    print("ğŸ” è¯¦ç»†äº¤å‰éªŒè¯æ¼”ç¤º")
    print("=" * 60)
    
    # 1. è·å–æ•°æ®
    print("\nğŸ“Š æ­¥éª¤1: è·å–æ‰€æœ‰é¢„æµ‹æ•°æ®")
    conn = sqlite3.connect("calibration.db")
    query = """
        SELECT prediction_date, symbol, predicted_probability, actual_direction
        FROM predictions 
        WHERE actual_direction IS NOT NULL
        ORDER BY prediction_date
    """
    df = pd.read_sql_query(query, conn)
    conn.close()
    
    print(f"æ€»æ•°æ®é‡: {len(df)} æ¡è®°å½•")
    print(f"æ—¶é—´èŒƒå›´: {df['prediction_date'].min()} ~ {df['prediction_date'].max()}")
    print(f"è‚¡ç¥¨æ•°é‡: {df['symbol'].nunique()} åª")
    
    # 2. è®¾ç½®äº¤å‰éªŒè¯å‚æ•°
    n_folds = 5
    fold_size = len(df) // n_folds
    print(f"\nğŸ”„ æ­¥éª¤2: è®¾ç½®äº¤å‰éªŒè¯å‚æ•°")
    print(f"æŠ˜æ•°: {n_folds}")
    print(f"æ¯æŠ˜å¤§å°: {fold_size} æ¡è®°å½•")
    
    # 3. æ‰§è¡Œäº¤å‰éªŒè¯
    print(f"\nâš™ï¸ æ­¥éª¤3: æ‰§è¡Œäº¤å‰éªŒè¯")
    print("-" * 60)
    
    platt_improvements = []
    isotonic_improvements = []
    fold_details = []
    
    for fold in range(n_folds):
        print(f"\nğŸ“‹ Fold {fold + 1}/{n_folds}")
        
        # è®¡ç®—æ•°æ®åˆ†å‰²ç‚¹
        train_end = fold * fold_size
        test_start = fold * fold_size
        test_end = (fold + 1) * fold_size
        
        if train_end < 50:  # è®­ç»ƒé›†å¤ªå°ï¼Œè·³è¿‡
            print("   âš ï¸ è®­ç»ƒé›†å¤ªå°ï¼Œè·³è¿‡")
            continue
            
        # åˆ†å‰²æ•°æ®
        train_df = df.iloc[:train_end]
        test_df = df.iloc[test_start:test_end]
        
        if len(test_df) == 0:
            print("   âš ï¸ æµ‹è¯•é›†ä¸ºç©ºï¼Œè·³è¿‡")
            continue
        
        print(f"   è®­ç»ƒé›†: {len(train_df)} æ¡ ({train_df['prediction_date'].min()} ~ {train_df['prediction_date'].max()})")
        print(f"   æµ‹è¯•é›†: {len(test_df)} æ¡ ({test_df['prediction_date'].min()} ~ {test_df['prediction_date'].max()})")
        
        # å‡†å¤‡æ•°æ®
        train_probs = train_df['predicted_probability'].values
        train_labels = train_df['actual_direction'].values
        test_probs = test_df['predicted_probability'].values
        test_labels = test_df['actual_direction'].values
        
        # æµ‹è¯•Platt Scaling
        try:
            # è®­ç»ƒæ ¡å‡†æ¨¡å‹
            epsilon = 1e-7
            train_probs_clipped = np.clip(train_probs, epsilon, 1 - epsilon)
            train_logits = np.log(train_probs_clipped / (1 - train_probs_clipped))
            
            platt_model = LogisticRegression()
            platt_model.fit(train_logits.reshape(-1, 1), train_labels)
            
            # åœ¨æµ‹è¯•é›†ä¸Šåº”ç”¨æ ¡å‡†
            test_probs_clipped = np.clip(test_probs, epsilon, 1 - epsilon)
            test_logits = np.log(test_probs_clipped / (1 - test_probs_clipped))
            calibrated_test_probs = platt_model.predict_proba(test_logits.reshape(-1, 1))[:, 1]
            
            # è®¡ç®—æ”¹è¿›
            original_brier = brier_score_loss(test_labels, test_probs)
            calibrated_brier = brier_score_loss(test_labels, calibrated_test_probs)
            platt_improvement = (original_brier - calibrated_brier) / original_brier * 100
            
            platt_improvements.append(platt_improvement)
            print(f"   Platt Scalingæ”¹è¿›: {platt_improvement:.2f}%")
            
        except Exception as e:
            print(f"   Platt Scalingå¤±è´¥: {e}")
            platt_improvement = 0
        
        # æµ‹è¯•Isotonic Regression
        try:
            isotonic_model = IsotonicRegression(out_of_bounds='clip')
            isotonic_model.fit(train_probs, train_labels)
            
            calibrated_test_probs_iso = isotonic_model.transform(test_probs)
            
            original_brier = brier_score_loss(test_labels, test_probs)
            calibrated_brier_iso = brier_score_loss(test_labels, calibrated_test_probs_iso)
            isotonic_improvement = (original_brier - calibrated_brier_iso) / original_brier * 100
            
            isotonic_improvements.append(isotonic_improvement)
            print(f"   Isotonic Regressionæ”¹è¿›: {isotonic_improvement:.2f}%")
            
        except Exception as e:
            print(f"   Isotonic Regressionå¤±è´¥: {e}")
            isotonic_improvement = 0
        
        # ä¿å­˜è¯¦æƒ…
        fold_details.append({
            'fold': fold + 1,
            'train_size': len(train_df),
            'test_size': len(test_df),
            'train_period': f"{train_df['prediction_date'].min()} ~ {train_df['prediction_date'].max()}",
            'test_period': f"{test_df['prediction_date'].min()} ~ {test_df['prediction_date'].max()}",
            'platt_improvement': platt_improvement,
            'isotonic_improvement': isotonic_improvement
        })
    
    # 4. æ±‡æ€»ç»“æœ
    print(f"\nğŸ“Š æ­¥éª¤4: æ±‡æ€»äº¤å‰éªŒè¯ç»“æœ")
    print("-" * 60)
    
    if platt_improvements:
        platt_mean = np.mean(platt_improvements)
        platt_std = np.std(platt_improvements)
        print(f"Platt Scaling:")
        print(f"  å¹³å‡æ”¹è¿›: {platt_mean:.2f}% Â± {platt_std:.2f}%")
        print(f"  æ”¹è¿›èŒƒå›´: {min(platt_improvements):.2f}% ~ {max(platt_improvements):.2f}%")
        print(f"  æœ‰æ•ˆæŠ˜æ•°: {len(platt_improvements)}")
    
    if isotonic_improvements:
        isotonic_mean = np.mean(isotonic_improvements)
        isotonic_std = np.std(isotonic_improvements)
        print(f"Isotonic Regression:")
        print(f"  å¹³å‡æ”¹è¿›: {isotonic_mean:.2f}% Â± {isotonic_std:.2f}%")
        print(f"  æ”¹è¿›èŒƒå›´: {min(isotonic_improvements):.2f}% ~ {max(isotonic_improvements):.2f}%")
        print(f"  æœ‰æ•ˆæŠ˜æ•°: {len(isotonic_improvements)}")
    
    # 5. è¯¦ç»†ç»“æœè¡¨
    print(f"\nğŸ“‹ æ­¥éª¤5: å„æŠ˜è¯¦ç»†ç»“æœ")
    print("-" * 80)
    print(f"{'Fold':<4} {'è®­ç»ƒé›†':<8} {'æµ‹è¯•é›†':<8} {'Plattæ”¹è¿›':<10} {'Isotonicæ”¹è¿›':<12} {'æµ‹è¯•æœŸé—´'}")
    print("-" * 80)
    
    for detail in fold_details:
        print(f"{detail['fold']:<4} {detail['train_size']:<8} {detail['test_size']:<8} "
              f"{detail['platt_improvement']:<10.2f} {detail['isotonic_improvement']:<12.2f} "
              f"{detail['test_period']}")
    
    # 6. è§£é‡Šäº¤å‰éªŒè¯çš„ä¼˜åŠ¿
    print(f"\nğŸ’¡ äº¤å‰éªŒè¯çš„ä¼˜åŠ¿")
    print("-" * 40)
    print("1. æ—¶é—´åºåˆ—åˆ†å‰²: ä¸¥æ ¼æŒ‰æ—¶é—´é¡ºåºï¼Œé¿å…æœªæ¥æ•°æ®æ³„éœ²")
    print("2. å¤šæ¬¡éªŒè¯: æ¯ä¸ªæ—¶é—´æ®µéƒ½ä½œä¸ºæµ‹è¯•é›†ï¼Œç»“æœæ›´å¯é ")
    print("3. ç»Ÿè®¡æ„ä¹‰: æä¾›å‡å€¼å’Œæ ‡å‡†å·®ï¼Œè¯„ä¼°ç¨³å®šæ€§")
    print("4. æ¸è¿›è®­ç»ƒ: æ¨¡æ‹ŸçœŸå®åœºæ™¯ï¼Œè®­ç»ƒé›†é€æ¸å¢å¤§")
    
    return {
        'platt_improvements': platt_improvements,
        'isotonic_improvements': isotonic_improvements,
        'fold_details': fold_details
    }

def visualize_cross_validation():
    """å¯è§†åŒ–äº¤å‰éªŒè¯è¿‡ç¨‹"""
    print("\nğŸ¨ å¯è§†åŒ–äº¤å‰éªŒè¯åˆ†å‰²")
    
    # è·å–æ•°æ®æ—¶é—´èŒƒå›´
    conn = sqlite3.connect("calibration.db")
    query = "SELECT prediction_date FROM predictions WHERE actual_direction IS NOT NULL ORDER BY prediction_date"
    dates_df = pd.read_sql_query(query, conn)
    conn.close()
    
    dates = pd.to_datetime(dates_df['prediction_date'])
    total_days = (dates.max() - dates.min()).days
    
    print(f"æ•°æ®æ—¶é—´è·¨åº¦: {total_days} å¤©")
    print(f"ä» {dates.min().date()} åˆ° {dates.max().date()}")
    
    # æ˜¾ç¤ºåˆ†å‰²ç¤ºæ„å›¾
    n_folds = 5
    fold_size = len(dates) // n_folds
    
    print(f"\nğŸ“… 5æŠ˜äº¤å‰éªŒè¯åˆ†å‰²ç¤ºæ„:")
    print("=" * 60)
    
    for fold in range(n_folds):
        train_end = fold * fold_size
        test_start = fold * fold_size
        test_end = (fold + 1) * fold_size
        
        if train_end < 50:
            continue
            
        train_dates = dates.iloc[:train_end]
        test_dates = dates.iloc[test_start:test_end]
        
        print(f"Fold {fold + 1}:")
        print(f"  è®­ç»ƒ: {train_dates.min().date()} ~ {train_dates.max().date()} ({len(train_dates)} æ¡)")
        print(f"  æµ‹è¯•: {test_dates.min().date()} ~ {test_dates.max().date()} ({len(test_dates)} æ¡)")
        print(f"  {'â–ˆ' * (len(train_dates) // 100)}{'â–‘' * (len(test_dates) // 100)}")
        print()

if __name__ == "__main__":
    # æ‰§è¡Œè¯¦ç»†æ¼”ç¤º
    results = detailed_cross_validation_demo()
    
    # å¯è§†åŒ–åˆ†å‰²è¿‡ç¨‹
    visualize_cross_validation()
    
    print("\nğŸ¯ æ€»ç»“:")
    print("äº¤å‰éªŒè¯é€šè¿‡æ—¶é—´åºåˆ—åˆ†å‰²ç¡®ä¿äº†:")
    print("- è®­ç»ƒé›†å§‹ç»ˆåœ¨æµ‹è¯•é›†ä¹‹å‰ï¼ˆæ—¶é—´é¡ºåºï¼‰")
    print("- æ¯æŠ˜éƒ½ç‹¬ç«‹æµ‹è¯•æ ¡å‡†æ•ˆæœ")
    print("- æä¾›ç¨³å¥çš„æ€§èƒ½ä¼°è®¡")
    print("- é¿å…è¿‡æ‹Ÿåˆå’Œæ•°æ®æ³„éœ²")












